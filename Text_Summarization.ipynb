{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b0bb09",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49b80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78c0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bc0fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seems', 'either', 'therein', 'herein', 'from', 'above', 'he', 'whence', 'except', 'call', 'where', 'have', 'everywhere', 'by', 'ca', '‘ve', 'very', 'yourself', '‘d', 'although', 'many', 'because', 'least', 'among', 'namely', 'since', 'twenty', 'through', \"n't\", 'whereby', 'around', 'not', 'full', 'using', 'beforehand', 'about', 'again', 'get', 'the', 'in', 'somehow', 'together', 'put', 'am', 'those', 'them', '‘ll', 'here', 'me', 'whoever', 'off', 'twelve', 'perhaps', 'please', 'done', 'third', 'next', 'forty', 'somewhere', 'whom', \"'ll\", 'while', 'thus', 'three', 'mostly', 'via', 'give', 'eight', 'else', 'amongst', 'often', 'onto', 'name', 'indeed', 'last', 'someone', 'seemed', 'will', 'toward', 'seeming', 'herself', 'doing', '’s', 'take', 'of', 'is', 'when', 'though', 'nor', 'most', 'an', 'other', 'our', 'several', 'once', 'himself', 'eleven', 'sixty', 'every', 'until', 'against', 'whereupon', 'front', \"'d\", 'towards', 'for', \"'re\", 'were', 'well', 'with', '‘re', 'anyone', 'along', 'been', 'now', 'out', 'i', 'therefore', 'they', 'almost', 'nobody', 'becoming', 'empty', 'something', 'under', 'his', 'as', 'being', 'nowhere', 'but', \"'ve\", 'made', 'thereupon', 'fifteen', 'be', 'during', 'former', 'per', 'some', 'that', 'everyone', 'itself', 'five', 'already', 'really', 'does', 'latterly', 'still', 'anything', 'wherever', 'she', 'back', 'hereby', 'yet', 'besides', 'go', 'beyond', 'ten', 'formerly', 'whose', 'another', 'without', 'these', 'over', 'we', 'had', 'or', 'neither', 'whereas', 'make', 'bottom', 'thru', 'various', 'whither', 'within', 'your', 'latter', 'rather', 'each', 'becomes', \"'s\", 'between', 'otherwise', 'elsewhere', 'could', 'none', 'do', 'any', 'both', '‘m', 'after', 'was', 'four', 'however', 'always', 'top', 'then', 'even', 'sometime', 'down', 'than', 'into', 'serious', '’m', 'no', 'so', 'own', 'at', 'which', 'there', 'might', 'enough', 'whereafter', 'never', 'anyway', 'anyhow', 'became', 'cannot', 'its', '’d', 'part', 'thereby', 'whatever', 'more', 'their', 'did', 'keep', 'us', 'quite', 'hundred', 'may', 'show', 'up', 'across', '’ll', 'seem', 'sometimes', 'wherein', 'throughout', 'it', 'hereupon', 'hereafter', 'less', 'regarding', 'all', 'you', 're', 'themselves', 'below', 'whenever', 'amount', 'anywhere', 'beside', 'would', 'first', 'nine', 'and', 'before', 'just', 'unless', 'too', 'her', 'must', 'n’t', 'are', 'become', 'what', 'everything', 'myself', 'one', 'yours', '‘s', 'two', 'such', 'also', 'side', 'should', 'used', 'him', 'whole', 'on', 'six', 'how', 'can', 'fifty', 'ever', 'noone', 'others', 'say', 'only', 'nothing', '’ve', 'few', 'thereafter', 'my', 'moreover', 'see', 'yourselves', 'hence', 'a', 'if', 'afterwards', 'alone', 'much', \"'m\", 'who', 'further', 'behind', 'move', 'upon', 'has', 'same', 'this', 'meanwhile', 'ourselves', 'due', 'n‘t', 'hers', 'ours', 'to', 'thence', 'mine', 'whether', '’re', 'nevertheless', 'why']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5275047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "591814d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text.\n",
    "\n",
    "It is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory.[2] GPT-3's full version has a capacity of 175 billion machine learning parameters. GPT-3, which was introduced in May 2020, and was in beta testing as of July 2020,[3] is part of a trend in natural language processing (NLP) systems of pre-trained language representations.[1]\n",
    "\n",
    "The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks.[4] Thirty-one OpenAI researchers and engineers presented the original May 28, 2020 paper introducing GPT-3. In their paper, they warned of GPT-3's potential dangers and called for research to mitigate risk.[1]: 34  David Chalmers, an Australian philosopher, described GPT-3 as \"one of the most interesting and important AI systems ever produced.\"[5]\n",
    "\n",
    "Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.[6]\n",
    "\n",
    "Further information: GPT-2 § Background\n",
    "According to The Economist, improved algorithms, powerful computers, and an increase in digitized data have fueled a revolution in machine learning, with new techniques in the 2010s resulting in \"rapid improvements in tasks\" including manipulating language.[7] Software models are trained to learn by using thousands or millions of examples in a \"structure ... loosely based on the neural architecture of the brain\".[7] One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was first introduced in 2017—the Transformer.[8] GPT-n models are based on this Transformer-based deep learning neural network architecture. There are a number of NLP systems capable of processing, mining, organizing, connecting, contrasting, understanding and generating answers to questions.[9]\n",
    "\n",
    "On June 11, 2018, OpenAI researchers and engineers posted their original paper on generative models—language models—artificial intelligence systems—that could be pre-trained with an enormous and diverse corpus of text via datasets, in a process they called generative pre-training (GP).[10] The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of \"generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\" This eliminated the need for human supervision and for time-intensive hand-labeling.[10]\n",
    "\n",
    "In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was claimed to be the \"largest language model ever published at 17 billion parameters.\"[11] It performed better than any other language model at a variety of tasks which included summarizing texts and answering questions, despite being smaller than IBM's Tangora language model which had over 8 trillion parameters.[11]\n",
    "\n",
    "Training and capabilities[edit]\n",
    "On May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the development of GPT-3, a third-generation \"state-of-the-art language model\".[1][4] The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[12] making GPT-3 the largest non-sparse (in a sparse model, many of those parameters are set to the same constant value, so even if there are more total parameters, there is less meaningful information) language model to date.[1]: 14 [2] Because GPT-3 is structurally similar to its predecessors,[1] its higher level of accuracy is attributed to its increased capacity and higher number of parameters.[13] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model.[4]\n",
    "\n",
    "Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]: 9  GPT-3 was trained on hundreds of billions of words and is capable of coding in CSS, JSX, Python, among others.[3]\n",
    "\n",
    "GPT-3 Training Data\n",
    "Dataset\t# Tokens\tWeight in Training Mix\n",
    "Common Crawl\t410 billion\t60%\n",
    "WebText2\t19 billion\t22%\n",
    "Books1\t12 billion\t8%\n",
    "Books2\t55 billion\t8%\n",
    "Wikipedia\t3 billion\t3%\n",
    "Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[3] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data. A study from the University of Washington found that GPT-3 produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT-2 and CTRL. GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[14]\n",
    "\n",
    "On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology.[15][16] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[15] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[17] In an initial experiment 80 US subjects were asked to judge if short ~200 word articles were written by humans or GPT-3. The participants judged incorrectly 48% of the time, doing only slightly better than random guessing.[1]\n",
    "\n",
    "Because GPT-3 can \"generate news articles which human evaluators have difficulty distinguishing from articles written by humans,\"[4] GPT-3 has the \"potential to advance both the beneficial and harmful applications of language models.\"[1]: 34  In their May 28, 2020 paper, the researchers described in detail the potential \"harmful effects of GPT-3\"[4] which include \"misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting\".[1] The authors draw attention to these dangers to call for research on risk mitigation.[1][18]: 34 \n",
    "\n",
    "GPT-3 is capable of performing zero-shot, few-shot and one-shot learning.[1]\n",
    "\n",
    "Reception[edit]\n",
    "Applications[edit]\n",
    "GPT-3, specifically the Codex model, is the basis for GitHub Copilot, a code completion and generation software that can be used in various code editors and IDEs.\n",
    "GPT-3 is used in certain Microsoft products to translate conventional language into formal computer code.[19]\n",
    "GPT-3 has been used by Andrew Mayne for AI Writer,[20] which allows people to correspond with historical figures via email.\n",
    "GPT-3 has been used by Jason Rohrer in a retro-themed chatbot project named \"Project December\", which is accessible online and allows users to converse with several AIs using GPT-3 technology.[21]\n",
    "GPT-3 was used by The Guardian to write an article about AI being harmless to human beings. It was fed some ideas and produced eight different essays, which were ultimately merged into one article.[22]\n",
    "GPT-3 is used in AI Dungeon, which generates text-based adventure games.\n",
    "Reviews[edit]\n",
    "In a July 2020 review in The New York Times, Farhad Manjoo said that GPT-3's ability to generate computer code, poetry, and prose is not just \"amazing\", \"spooky\", and \"humbling\", but also \"more than a little terrifying\".[23]\n",
    "Daily Nous presented a series of articles by nine philosophers on GPT-3.[24] Australian philosopher David Chalmers described GPT-3 as \"one of the most interesting and important AI systems ever produced\".[5]\n",
    "A review in Wired said that GPT-3 was \"provoking chills across Silicon Valley\".[25]\n",
    "The National Law Review said that GPT-3 is an \"impressive step in the larger process\", with OpenAI and others finding \"useful applications for all of this power\" while continuing to \"work toward a more general intelligence\".[26]\n",
    "An article in the MIT Technology Review, cowritten by Deep Learning critic Gary Marcus,[27] stated that GPT-3's \"comprehension of the world is often seriously off, which means you can never really trust what it says.\"[28] According to the authors, GPT-3 models relationships between words without having an understanding of the meaning behind each word.\n",
    "Jerome Pesenti, head of the Facebook AI lab, said GPT-3 is \"unsafe,\" pointing to the sexist, racist and other biased and negative language generated by the system when it was asked to discuss Jews, women, black people, and the Holocaust.[29]\n",
    "Nabla, a French start-up specializing in healthcare technology, tested GPT-3 as a medical chatbot, though OpenAI itself warned against such use. As expected, GPT-3 showed several limitations. For example, while testing GPT-3 responses about mental health issues, the AI advised a simulated patient to commit suicide.[30]\n",
    "Noam Chomsky expressed his skepticism about GPT-3's scientific value: \"It's not a language model. It works just as well for impossible languages as for actual languages. It is therefore refuted, if intended as a language model, by normal scientific criteria. [...] Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition generally.\"[31]\n",
    "Luciano Floridi and Massimo Chiriatti highlighted the risk of \"cheap production of good, semantic artefacts\".[32]\n",
    "Criticism[edit]\n",
    "GPT-3's builder, OpenAI, was initially founded as a non-profit in 2015.[33] In 2019, OpenAI did not publicly release GPT-3's precursor model, breaking from OpenAI's previous open-source practices, citing concerns that the model would perpetuate fake news. OpenAI eventually released a version of GPT-2 that was 8% of the original model's size.[34] In the same year, OpenAI restructured to be a for-profit company.[35] In 2020, Microsoft announced the company had exclusive licensing of GPT-3 for Microsoft's products and services following a multi-billion dollar investment in OpenAI. The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to the GPT-3's source code.[6]\n",
    "\n",
    "Large language models, such as GPT-3, have come under criticism from Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[36]\n",
    "\n",
    "The growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[37] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[38]\n",
    "\n",
    "GPT-3 was criticized for its algorithmic bias; for example, it is more likely to associate Islam with terrorism.[39]\n",
    "\n",
    "In its response to the Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office (\"USPTO\"), OpenAI acknowledges that \"copyright protection arises automatically when an author creates an original work and fixes it in a tangible medium, see 17 U.S.C. § 102, the vast majority of content posted online is protected by U.S. copyright laws.[40] However, GPT was built with data from the Common Crawl dataset, a conglomerate of copyrighted articles, internet posts, web pages, and books scraped from 60 million domains over a period of 12 years. TechCrunch reports this data includes copyrighted material from BBC, The New York Times, Reddit, the full text of online books, and more.[41] All GPT-based products therefore run the risk of plagiarism. In April 2021, a group of computer scientists used a tool that identifies text generated by GPT in an effort to isolate the reason for strange phrases appearing in scientific papers. Cabanac and colleagues ran a selection of abstracts from Microprocessors and Microsystems through this tool and discovered \"critical flaws\", such as nonsensical text and plagiarized text and images.[42]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f84ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2441b918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text.\n",
       "\n",
       "It is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory.[2] GPT-3's full version has a capacity of 175 billion machine learning parameters. GPT-3, which was introduced in May 2020, and was in beta testing as of July 2020,[3] is part of a trend in natural language processing (NLP) systems of pre-trained language representations.[1]\n",
       "\n",
       "The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks.[4] Thirty-one OpenAI researchers and engineers presented the original May 28, 2020 paper introducing GPT-3. In their paper, they warned of GPT-3's potential dangers and called for research to mitigate risk.[1]: 34  David Chalmers, an Australian philosopher, described GPT-3 as \"one of the most interesting and important AI systems ever produced.\"[5]\n",
       "\n",
       "Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.[6]\n",
       "\n",
       "Further information: GPT-2 § Background\n",
       "According to The Economist, improved algorithms, powerful computers, and an increase in digitized data have fueled a revolution in machine learning, with new techniques in the 2010s resulting in \"rapid improvements in tasks\" including manipulating language.[7] Software models are trained to learn by using thousands or millions of examples in a \"structure ... loosely based on the neural architecture of the brain\".[7] One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was first introduced in 2017—the Transformer.[8] GPT-n models are based on this Transformer-based deep learning neural network architecture. There are a number of NLP systems capable of processing, mining, organizing, connecting, contrasting, understanding and generating answers to questions.[9]\n",
       "\n",
       "On June 11, 2018, OpenAI researchers and engineers posted their original paper on generative models—language models—artificial intelligence systems—that could be pre-trained with an enormous and diverse corpus of text via datasets, in a process they called generative pre-training (GP).[10] The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of \"generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\" This eliminated the need for human supervision and for time-intensive hand-labeling.[10]\n",
       "\n",
       "In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was claimed to be the \"largest language model ever published at 17 billion parameters.\"[11] It performed better than any other language model at a variety of tasks which included summarizing texts and answering questions, despite being smaller than IBM's Tangora language model which had over 8 trillion parameters.[11]\n",
       "\n",
       "Training and capabilities[edit]\n",
       "On May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the development of GPT-3, a third-generation \"state-of-the-art language model\".[1][4] The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[12] making GPT-3 the largest non-sparse (in a sparse model, many of those parameters are set to the same constant value, so even if there are more total parameters, there is less meaningful information) language model to date.[1]: 14 [2] Because GPT-3 is structurally similar to its predecessors,[1] its higher level of accuracy is attributed to its increased capacity and higher number of parameters.[13] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model.[4]\n",
       "\n",
       "Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]: 9  GPT-3 was trained on hundreds of billions of words and is capable of coding in CSS, JSX, Python, among others.[3]\n",
       "\n",
       "GPT-3 Training Data\n",
       "Dataset\t# Tokens\tWeight in Training Mix\n",
       "Common Crawl\t410 billion\t60%\n",
       "WebText2\t19 billion\t22%\n",
       "Books1\t12 billion\t8%\n",
       "Books2\t55 billion\t8%\n",
       "Wikipedia\t3 billion\t3%\n",
       "Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[3] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data. A study from the University of Washington found that GPT-3 produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT-2 and CTRL. GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[14]\n",
       "\n",
       "On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology.[15][16] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[15] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[17] In an initial experiment 80 US subjects were asked to judge if short ~200 word articles were written by humans or GPT-3. The participants judged incorrectly 48% of the time, doing only slightly better than random guessing.[1]\n",
       "\n",
       "Because GPT-3 can \"generate news articles which human evaluators have difficulty distinguishing from articles written by humans,\"[4] GPT-3 has the \"potential to advance both the beneficial and harmful applications of language models.\"[1]: 34  In their May 28, 2020 paper, the researchers described in detail the potential \"harmful effects of GPT-3\"[4] which include \"misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting\".[1] The authors draw attention to these dangers to call for research on risk mitigation.[1][18]: 34 \n",
       "\n",
       "GPT-3 is capable of performing zero-shot, few-shot and one-shot learning.[1]\n",
       "\n",
       "Reception[edit]\n",
       "Applications[edit]\n",
       "GPT-3, specifically the Codex model, is the basis for GitHub Copilot, a code completion and generation software that can be used in various code editors and IDEs.\n",
       "GPT-3 is used in certain Microsoft products to translate conventional language into formal computer code.[19]\n",
       "GPT-3 has been used by Andrew Mayne for AI Writer,[20] which allows people to correspond with historical figures via email.\n",
       "GPT-3 has been used by Jason Rohrer in a retro-themed chatbot project named \"Project December\", which is accessible online and allows users to converse with several AIs using GPT-3 technology.[21]\n",
       "GPT-3 was used by The Guardian to write an article about AI being harmless to human beings. It was fed some ideas and produced eight different essays, which were ultimately merged into one article.[22]\n",
       "GPT-3 is used in AI Dungeon, which generates text-based adventure games.\n",
       "Reviews[edit]\n",
       "In a July 2020 review in The New York Times, Farhad Manjoo said that GPT-3's ability to generate computer code, poetry, and prose is not just \"amazing\", \"spooky\", and \"humbling\", but also \"more than a little terrifying\".[23]\n",
       "Daily Nous presented a series of articles by nine philosophers on GPT-3.[24] Australian philosopher David Chalmers described GPT-3 as \"one of the most interesting and important AI systems ever produced\".[5]\n",
       "A review in Wired said that GPT-3 was \"provoking chills across Silicon Valley\".[25]\n",
       "The National Law Review said that GPT-3 is an \"impressive step in the larger process\", with OpenAI and others finding \"useful applications for all of this power\" while continuing to \"work toward a more general intelligence\".[26]\n",
       "An article in the MIT Technology Review, cowritten by Deep Learning critic Gary Marcus,[27] stated that GPT-3's \"comprehension of the world is often seriously off, which means you can never really trust what it says.\"[28] According to the authors, GPT-3 models relationships between words without having an understanding of the meaning behind each word.\n",
       "Jerome Pesenti, head of the Facebook AI lab, said GPT-3 is \"unsafe,\" pointing to the sexist, racist and other biased and negative language generated by the system when it was asked to discuss Jews, women, black people, and the Holocaust.[29]\n",
       "Nabla, a French start-up specializing in healthcare technology, tested GPT-3 as a medical chatbot, though OpenAI itself warned against such use. As expected, GPT-3 showed several limitations. For example, while testing GPT-3 responses about mental health issues, the AI advised a simulated patient to commit suicide.[30]\n",
       "Noam Chomsky expressed his skepticism about GPT-3's scientific value: \"It's not a language model. It works just as well for impossible languages as for actual languages. It is therefore refuted, if intended as a language model, by normal scientific criteria. [...] Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition generally.\"[31]\n",
       "Luciano Floridi and Massimo Chiriatti highlighted the risk of \"cheap production of good, semantic artefacts\".[32]\n",
       "Criticism[edit]\n",
       "GPT-3's builder, OpenAI, was initially founded as a non-profit in 2015.[33] In 2019, OpenAI did not publicly release GPT-3's precursor model, breaking from OpenAI's previous open-source practices, citing concerns that the model would perpetuate fake news. OpenAI eventually released a version of GPT-2 that was 8% of the original model's size.[34] In the same year, OpenAI restructured to be a for-profit company.[35] In 2020, Microsoft announced the company had exclusive licensing of GPT-3 for Microsoft's products and services following a multi-billion dollar investment in OpenAI. The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to the GPT-3's source code.[6]\n",
       "\n",
       "Large language models, such as GPT-3, have come under criticism from Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[36]\n",
       "\n",
       "The growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[37] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[38]\n",
       "\n",
       "GPT-3 was criticized for its algorithmic bias; for example, it is more likely to associate Islam with terrorism.[39]\n",
       "\n",
       "In its response to the Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office (\"USPTO\"), OpenAI acknowledges that \"copyright protection arises automatically when an author creates an original work and fixes it in a tangible medium, see 17 U.S.C. § 102, the vast majority of content posted online is protected by U.S. copyright laws.[40] However, GPT was built with data from the Common Crawl dataset, a conglomerate of copyrighted articles, internet posts, web pages, and books scraped from 60 million domains over a period of 12 years. TechCrunch reports this data includes copyrighted material from BBC, The New York Times, Reddit, the full text of online books, and more.[41] All GPT-based products therefore run the risk of plagiarism. In April 2021, a group of computer scientists used a tool that identifies text generated by GPT in an effort to isolate the reason for strange phrases appearing in scientific papers. Cabanac and colleagues ran a selection of abstracts from Microprocessors and Microsystems through this tool and discovered \"critical flaws\", such as nonsensical text and plagiarized text and images.[42]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354036ae",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ec21197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'Generative', 'Pre', '-', 'trained', 'Transformer', '3', '(', 'GPT-3', ')', 'is', 'an', 'autoregressive', 'language', 'model', 'that', 'uses', 'deep', 'learning', 'to', 'produce', 'human', '-', 'like', 'text', '.', '\\n\\n', 'It', 'is', 'the', 'third', '-', 'generation', 'language', 'prediction', 'model', 'in', 'the', 'GPT', '-', 'n', 'series', '(', 'and', 'the', 'successor', 'to', 'GPT-2', ')', 'created', 'by', 'OpenAI', ',', 'a', 'San', 'Francisco', '-', 'based', 'artificial', 'intelligence', 'research', 'laboratory.[2', ']', 'GPT-3', \"'s\", 'full', 'version', 'has', 'a', 'capacity', 'of', '175', 'billion', 'machine', 'learning', 'parameters', '.', 'GPT-3', ',', 'which', 'was', 'introduced', 'in', 'May', '2020', ',', 'and', 'was', 'in', 'beta', 'testing', 'as', 'of', 'July', '2020,[3', ']', 'is', 'part', 'of', 'a', 'trend', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'systems', 'of', 'pre', '-', 'trained', 'language', 'representations.[1', ']', '\\n\\n', 'The', 'quality', 'of', 'the', 'text', 'generated', 'by', 'GPT-3', 'is', 'so', 'high', 'that', 'it', 'can', 'be', 'difficult', 'to', 'determine', 'whether', 'or', 'not', 'it', 'was', 'written', 'by', 'a', 'human', ',', 'which', 'has', 'both', 'benefits', 'and', 'risks.[4', ']', 'Thirty', '-', 'one', 'OpenAI', 'researchers', 'and', 'engineers', 'presented', 'the', 'original', 'May', '28', ',', '2020', 'paper', 'introducing', 'GPT-3', '.', 'In', 'their', 'paper', ',', 'they', 'warned', 'of', 'GPT-3', \"'s\", 'potential', 'dangers', 'and', 'called', 'for', 'research', 'to', 'mitigate', 'risk.[1', ']', ':', '\\u200a', '34', '\\u200a ', 'David', 'Chalmers', ',', 'an', 'Australian', 'philosopher', ',', 'described', 'GPT-3', 'as', '\"', 'one', 'of', 'the', 'most', 'interesting', 'and', 'important', 'AI', 'systems', 'ever', 'produced', '.', '\"[5', ']', '\\n\\n', 'Microsoft', 'announced', 'on', 'September', '22', ',', '2020', ',', 'that', 'it', 'had', 'licensed', '\"', 'exclusive', '\"', 'use', 'of', 'GPT-3', ';', 'others', 'can', 'still', 'use', 'the', 'public', 'API', 'to', 'receive', 'output', ',', 'but', 'only', 'Microsoft', 'has', 'access', 'to', 'GPT-3', \"'s\", 'underlying', 'model.[6', ']', '\\n\\n', 'Further', 'information', ':', 'GPT-2', '§', 'Background', '\\n', 'According', 'to', 'The', 'Economist', ',', 'improved', 'algorithms', ',', 'powerful', 'computers', ',', 'and', 'an', 'increase', 'in', 'digitized', 'data', 'have', 'fueled', 'a', 'revolution', 'in', 'machine', 'learning', ',', 'with', 'new', 'techniques', 'in', 'the', '2010s', 'resulting', 'in', '\"', 'rapid', 'improvements', 'in', 'tasks', '\"', 'including', 'manipulating', 'language.[7', ']', 'Software', 'models', 'are', 'trained', 'to', 'learn', 'by', 'using', 'thousands', 'or', 'millions', 'of', 'examples', 'in', 'a', '\"', 'structure', '...', 'loosely', 'based', 'on', 'the', 'neural', 'architecture', 'of', 'the', 'brain\".[7', ']', 'One', 'architecture', 'used', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'neural', 'network', 'based', 'on', 'a', 'deep', 'learning', 'model', 'that', 'was', 'first', 'introduced', 'in', '2017', '—', 'the', 'Transformer.[8', ']', 'GPT', '-', 'n', 'models', 'are', 'based', 'on', 'this', 'Transformer', '-', 'based', 'deep', 'learning', 'neural', 'network', 'architecture', '.', 'There', 'are', 'a', 'number', 'of', 'NLP', 'systems', 'capable', 'of', 'processing', ',', 'mining', ',', 'organizing', ',', 'connecting', ',', 'contrasting', ',', 'understanding', 'and', 'generating', 'answers', 'to', 'questions.[9', ']', '\\n\\n', 'On', 'June', '11', ',', '2018', ',', 'OpenAI', 'researchers', 'and', 'engineers', 'posted', 'their', 'original', 'paper', 'on', 'generative', 'models', '—', 'language', 'models', '—', 'artificial', 'intelligence', 'systems', '—', 'that', 'could', 'be', 'pre', '-', 'trained', 'with', 'an', 'enormous', 'and', 'diverse', 'corpus', 'of', 'text', 'via', 'datasets', ',', 'in', 'a', 'process', 'they', 'called', 'generative', 'pre', '-', 'training', '(', 'GP).[10', ']', 'The', 'authors', 'described', 'how', 'language', 'understanding', 'performances', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', 'were', 'improved', 'in', 'GPT', '-', 'n', 'through', 'a', 'process', 'of', '\"', 'generative', 'pre', '-', 'training', 'of', 'a', 'language', 'model', 'on', 'a', 'diverse', 'corpus', 'of', 'unlabeled', 'text', ',', 'followed', 'by', 'discriminative', 'fine', '-', 'tuning', 'on', 'each', 'specific', 'task', '.', '\"', 'This', 'eliminated', 'the', 'need', 'for', 'human', 'supervision', 'and', 'for', 'time', '-', 'intensive', 'hand', '-', 'labeling.[10', ']', '\\n\\n', 'In', 'February', '2020', ',', 'Microsoft', 'introduced', 'its', 'Turing', 'Natural', 'Language', 'Generation', '(', 'T', '-', 'NLG', ')', ',', 'which', 'was', 'claimed', 'to', 'be', 'the', '\"', 'largest', 'language', 'model', 'ever', 'published', 'at', '17', 'billion', 'parameters', '.', '\"[11', ']', 'It', 'performed', 'better', 'than', 'any', 'other', 'language', 'model', 'at', 'a', 'variety', 'of', 'tasks', 'which', 'included', 'summarizing', 'texts', 'and', 'answering', 'questions', ',', 'despite', 'being', 'smaller', 'than', 'IBM', \"'s\", 'Tangora', 'language', 'model', 'which', 'had', 'over', '8', 'trillion', 'parameters.[11', ']', '\\n\\n', 'Training', 'and', 'capabilities[edit', ']', '\\n', 'On', 'May', '28', ',', '2020', ',', 'an', 'arXiv', 'preprint', 'by', 'a', 'group', 'of', '31', 'engineers', 'and', 'researchers', 'at', 'OpenAI', 'described', 'the', 'development', 'of', 'GPT-3', ',', 'a', 'third', '-', 'generation', '\"', 'state', '-', 'of', '-', 'the', '-', 'art', 'language', 'model\".[1][4', ']', 'The', 'team', 'increased', 'the', 'capacity', 'of', 'GPT-3', 'by', 'over', 'two', 'orders', 'of', 'magnitude', 'from', 'that', 'of', 'its', 'predecessor', ',', 'GPT-2,[12', ']', 'making', 'GPT-3', 'the', 'largest', 'non', '-', 'sparse', '(', 'in', 'a', 'sparse', 'model', ',', 'many', 'of', 'those', 'parameters', 'are', 'set', 'to', 'the', 'same', 'constant', 'value', ',', 'so', 'even', 'if', 'there', 'are', 'more', 'total', 'parameters', ',', 'there', 'is', 'less', 'meaningful', 'information', ')', 'language', 'model', 'to', 'date.[1', ']', ':', '\\u200a', '14', '\\u200a', '[', '2', ']', 'Because', 'GPT-3', 'is', 'structurally', 'similar', 'to', 'its', 'predecessors,[1', ']', 'its', 'higher', 'level', 'of', 'accuracy', 'is', 'attributed', 'to', 'its', 'increased', 'capacity', 'and', 'higher', 'number', 'of', 'parameters.[13', ']', 'GPT-3', \"'s\", 'capacity', 'is', 'ten', 'times', 'larger', 'than', 'that', 'of', 'Microsoft', \"'s\", 'Turing', 'NLG', ',', 'the', 'next', 'largest', 'NLP', 'model.[4', ']', '\\n\\n', 'Sixty', 'percent', 'of', 'the', 'weighted', 'pre', '-', 'training', 'dataset', 'for', 'GPT-3', 'comes', 'from', 'a', 'filtered', 'version', 'of', 'Common', 'Crawl', 'consisting', 'of', '410', 'billion', 'byte', '-', 'pair', '-', 'encoded', 'tokens.[1', ']', ':', '\\u200a', '9', '\\u200a ', 'Other', 'sources', 'are', '19', 'billion', 'tokens', 'from', 'WebText2', 'representing', '22', '%', 'of', 'the', 'weighted', 'total', ',', '12', 'billion', 'tokens', 'from', 'Books1', 'representing', '8', '%', ',', '55', 'billion', 'tokens', 'from', 'Books2', 'representing', '8', '%', ',', 'and', '3', 'billion', 'tokens', 'from', 'Wikipedia', 'representing', '3%.[1', ']', ':', '\\u200a', '9', '\\u200a ', 'GPT-3', 'was', 'trained', 'on', 'hundreds', 'of', 'billions', 'of', 'words', 'and', 'is', 'capable', 'of', 'coding', 'in', 'CSS', ',', 'JSX', ',', 'Python', ',', 'among', 'others.[3', ']', '\\n\\n', 'GPT-3', 'Training', 'Data', '\\n', 'Dataset', '\\t', '#', 'Tokens', '\\t', 'Weight', 'in', 'Training', 'Mix', '\\n', 'Common', 'Crawl', '\\t', '410', 'billion', '\\t', '60', '%', '\\n', 'WebText2', '\\t', '19', 'billion', '\\t', '22', '%', '\\n', 'Books1', '\\t', '12', 'billion', '\\t', '8', '%', '\\n', 'Books2', '\\t', '55', 'billion', '\\t', '8', '%', '\\n', 'Wikipedia', '\\t', '3', 'billion', '\\t', '3', '%', '\\n', 'Since', 'GPT-3', \"'s\", 'training', 'data', 'was', 'all', '-', 'encompassing', ',', 'it', 'does', 'not', 'require', 'further', 'training', 'for', 'distinct', 'language', 'tasks.[3', ']', 'The', 'training', 'data', 'contains', 'occasional', 'toxic', 'language', 'and', 'GPT-3', 'occasionally', 'generates', 'toxic', 'language', 'as', 'a', 'result', 'of', 'mimicking', 'its', 'training', 'data', '.', 'A', 'study', 'from', 'the', 'University', 'of', 'Washington', 'found', 'that', 'GPT-3', 'produced', 'toxic', 'language', 'at', 'a', 'toxicity', 'level', 'comparable', 'to', 'the', 'similar', 'natural', 'language', 'processing', 'models', 'of', 'GPT-2', 'and', 'CTRL', '.', 'GPT-3', 'produced', 'less', 'toxic', 'language', 'compared', 'to', 'its', 'predecessor', 'model', ',', 'GPT-1', ',', 'although', 'it', 'produced', 'both', 'more', 'generations', 'and', 'a', 'higher', 'toxicity', 'of', 'toxic', 'language', 'compared', 'to', 'CTRL', 'Wiki', ',', 'a', 'language', 'model', 'trained', 'entirely', 'on', 'Wikipedia', 'data.[14', ']', '\\n\\n', 'On', 'June', '11', ',', '2020', ',', 'OpenAI', 'announced', 'that', 'users', 'could', 'request', 'access', 'to', 'its', 'user', '-', 'friendly', 'GPT-3', 'API', '—', 'a', '\"', 'machine', 'learning', 'toolset\"—to', 'help', 'OpenAI', '\"', 'explore', 'the', 'strengths', 'and', 'limits', '\"', 'of', 'this', 'new', 'technology.[15][16', ']', 'The', 'invitation', 'described', 'how', 'this', 'API', 'had', 'a', 'general', '-', 'purpose', '\"', 'text', 'in', ',', 'text', 'out', '\"', 'interface', 'that', 'can', 'complete', 'almost', '\"', 'any', 'English', 'language', 'task', '\"', ',', 'instead', 'of', 'the', 'usual', 'single', 'use', '-', 'case.[15', ']', 'According', 'to', 'one', 'user', ',', 'who', 'had', 'access', 'to', 'a', 'private', 'early', 'release', 'of', 'the', 'OpenAI', 'GPT-3', 'API', ',', 'GPT-3', 'was', '\"', 'eerily', 'good', '\"', 'at', 'writing', '\"', 'amazingly', 'coherent', 'text', '\"', 'with', 'only', 'a', 'few', 'simple', 'prompts.[17', ']', 'In', 'an', 'initial', 'experiment', '80', 'US', 'subjects', 'were', 'asked', 'to', 'judge', 'if', 'short', '~200', 'word', 'articles', 'were', 'written', 'by', 'humans', 'or', 'GPT-3', '.', 'The', 'participants', 'judged', 'incorrectly', '48', '%', 'of', 'the', 'time', ',', 'doing', 'only', 'slightly', 'better', 'than', 'random', 'guessing.[1', ']', '\\n\\n', 'Because', 'GPT-3', 'can', '\"', 'generate', 'news', 'articles', 'which', 'human', 'evaluators', 'have', 'difficulty', 'distinguishing', 'from', 'articles', 'written', 'by', 'humans,\"[4', ']', 'GPT-3', 'has', 'the', '\"', 'potential', 'to', 'advance', 'both', 'the', 'beneficial', 'and', 'harmful', 'applications', 'of', 'language', 'models', '.', '\"[1', ']', ':', '\\u200a', '34', '\\u200a ', 'In', 'their', 'May', '28', ',', '2020', 'paper', ',', 'the', 'researchers', 'described', 'in', 'detail', 'the', 'potential', '\"', 'harmful', 'effects', 'of', 'GPT-3\"[4', ']', 'which', 'include', '\"', 'misinformation', ',', 'spam', ',', 'phishing', ',', 'abuse', 'of', 'legal', 'and', 'governmental', 'processes', ',', 'fraudulent', 'academic', 'essay', 'writing', 'and', 'social', 'engineering', 'pretexting\".[1', ']', 'The', 'authors', 'draw', 'attention', 'to', 'these', 'dangers', 'to', 'call', 'for', 'research', 'on', 'risk', 'mitigation.[1][18', ']', ':', '\\u200a', '34', '\\u200a\\n\\n', 'GPT-3', 'is', 'capable', 'of', 'performing', 'zero', '-', 'shot', ',', 'few', '-', 'shot', 'and', 'one', '-', 'shot', 'learning.[1', ']', '\\n\\n', 'Reception[edit', ']', '\\n', 'Applications[edit', ']', '\\n', 'GPT-3', ',', 'specifically', 'the', 'Codex', 'model', ',', 'is', 'the', 'basis', 'for', 'GitHub', 'Copilot', ',', 'a', 'code', 'completion', 'and', 'generation', 'software', 'that', 'can', 'be', 'used', 'in', 'various', 'code', 'editors', 'and', 'IDEs', '.', '\\n', 'GPT-3', 'is', 'used', 'in', 'certain', 'Microsoft', 'products', 'to', 'translate', 'conventional', 'language', 'into', 'formal', 'computer', 'code.[19', ']', '\\n', 'GPT-3', 'has', 'been', 'used', 'by', 'Andrew', 'Mayne', 'for', 'AI', 'Writer,[20', ']', 'which', 'allows', 'people', 'to', 'correspond', 'with', 'historical', 'figures', 'via', 'email', '.', '\\n', 'GPT-3', 'has', 'been', 'used', 'by', 'Jason', 'Rohrer', 'in', 'a', 'retro', '-', 'themed', 'chatbot', 'project', 'named', '\"', 'Project', 'December', '\"', ',', 'which', 'is', 'accessible', 'online', 'and', 'allows', 'users', 'to', 'converse', 'with', 'several', 'AIs', 'using', 'GPT-3', 'technology.[21', ']', '\\n', 'GPT-3', 'was', 'used', 'by', 'The', 'Guardian', 'to', 'write', 'an', 'article', 'about', 'AI', 'being', 'harmless', 'to', 'human', 'beings', '.', 'It', 'was', 'fed', 'some', 'ideas', 'and', 'produced', 'eight', 'different', 'essays', ',', 'which', 'were', 'ultimately', 'merged', 'into', 'one', 'article.[22', ']', '\\n', 'GPT-3', 'is', 'used', 'in', 'AI', 'Dungeon', ',', 'which', 'generates', 'text', '-', 'based', 'adventure', 'games', '.', '\\n', 'Reviews[edit', ']', '\\n', 'In', 'a', 'July', '2020', 'review', 'in', 'The', 'New', 'York', 'Times', ',', 'Farhad', 'Manjoo', 'said', 'that', 'GPT-3', \"'s\", 'ability', 'to', 'generate', 'computer', 'code', ',', 'poetry', ',', 'and', 'prose', 'is', 'not', 'just', '\"', 'amazing', '\"', ',', '\"', 'spooky', '\"', ',', 'and', '\"', 'humbling', '\"', ',', 'but', 'also', '\"', 'more', 'than', 'a', 'little', 'terrifying\".[23', ']', '\\n', 'Daily', 'Nous', 'presented', 'a', 'series', 'of', 'articles', 'by', 'nine', 'philosophers', 'on', 'GPT-3.[24', ']', 'Australian', 'philosopher', 'David', 'Chalmers', 'described', 'GPT-3', 'as', '\"', 'one', 'of', 'the', 'most', 'interesting', 'and', 'important', 'AI', 'systems', 'ever', 'produced\".[5', ']', '\\n', 'A', 'review', 'in', 'Wired', 'said', 'that', 'GPT-3', 'was', '\"', 'provoking', 'chills', 'across', 'Silicon', 'Valley\".[25', ']', '\\n', 'The', 'National', 'Law', 'Review', 'said', 'that', 'GPT-3', 'is', 'an', '\"', 'impressive', 'step', 'in', 'the', 'larger', 'process', '\"', ',', 'with', 'OpenAI', 'and', 'others', 'finding', '\"', 'useful', 'applications', 'for', 'all', 'of', 'this', 'power', '\"', 'while', 'continuing', 'to', '\"', 'work', 'toward', 'a', 'more', 'general', 'intelligence\".[26', ']', '\\n', 'An', 'article', 'in', 'the', 'MIT', 'Technology', 'Review', ',', 'cowritten', 'by', 'Deep', 'Learning', 'critic', 'Gary', 'Marcus,[27', ']', 'stated', 'that', 'GPT-3', \"'s\", '\"', 'comprehension', 'of', 'the', 'world', 'is', 'often', 'seriously', 'off', ',', 'which', 'means', 'you', 'can', 'never', 'really', 'trust', 'what', 'it', 'says', '.', '\"[28', ']', 'According', 'to', 'the', 'authors', ',', 'GPT-3', 'models', 'relationships', 'between', 'words', 'without', 'having', 'an', 'understanding', 'of', 'the', 'meaning', 'behind', 'each', 'word', '.', '\\n', 'Jerome', 'Pesenti', ',', 'head', 'of', 'the', 'Facebook', 'AI', 'lab', ',', 'said', 'GPT-3', 'is', '\"', 'unsafe', ',', '\"', 'pointing', 'to', 'the', 'sexist', ',', 'racist', 'and', 'other', 'biased', 'and', 'negative', 'language', 'generated', 'by', 'the', 'system', 'when', 'it', 'was', 'asked', 'to', 'discuss', 'Jews', ',', 'women', ',', 'black', 'people', ',', 'and', 'the', 'Holocaust.[29', ']', '\\n', 'Nabla', ',', 'a', 'French', 'start', '-', 'up', 'specializing', 'in', 'healthcare', 'technology', ',', 'tested', 'GPT-3', 'as', 'a', 'medical', 'chatbot', ',', 'though', 'OpenAI', 'itself', 'warned', 'against', 'such', 'use', '.', 'As', 'expected', ',', 'GPT-3', 'showed', 'several', 'limitations', '.', 'For', 'example', ',', 'while', 'testing', 'GPT-3', 'responses', 'about', 'mental', 'health', 'issues', ',', 'the', 'AI', 'advised', 'a', 'simulated', 'patient', 'to', 'commit', 'suicide.[30', ']', '\\n', 'Noam', 'Chomsky', 'expressed', 'his', 'skepticism', 'about', 'GPT-3', \"'s\", 'scientific', 'value', ':', '\"', 'It', \"'s\", 'not', 'a', 'language', 'model', '.', 'It', 'works', 'just', 'as', 'well', 'for', 'impossible', 'languages', 'as', 'for', 'actual', 'languages', '.', 'It', 'is', 'therefore', 'refuted', ',', 'if', 'intended', 'as', 'a', 'language', 'model', ',', 'by', 'normal', 'scientific', 'criteria', '.', '[', '...', ']', 'Perhaps', 'it', \"'s\", 'useful', 'for', 'some', 'purpose', ',', 'but', 'it', 'seems', 'to', 'tell', 'us', 'nothing', 'about', 'language', 'or', 'cognition', 'generally', '.', '\"[31', ']', '\\n', 'Luciano', 'Floridi', 'and', 'Massimo', 'Chiriatti', 'highlighted', 'the', 'risk', 'of', '\"', 'cheap', 'production', 'of', 'good', ',', 'semantic', 'artefacts\".[32', ']', '\\n', 'Criticism[edit', ']', '\\n', 'GPT-3', \"'s\", 'builder', ',', 'OpenAI', ',', 'was', 'initially', 'founded', 'as', 'a', 'non', '-', 'profit', 'in', '2015.[33', ']', 'In', '2019', ',', 'OpenAI', 'did', 'not', 'publicly', 'release', 'GPT-3', \"'s\", 'precursor', 'model', ',', 'breaking', 'from', 'OpenAI', \"'s\", 'previous', 'open', '-', 'source', 'practices', ',', 'citing', 'concerns', 'that', 'the', 'model', 'would', 'perpetuate', 'fake', 'news', '.', 'OpenAI', 'eventually', 'released', 'a', 'version', 'of', 'GPT-2', 'that', 'was', '8', '%', 'of', 'the', 'original', 'model', \"'s\", 'size.[34', ']', 'In', 'the', 'same', 'year', ',', 'OpenAI', 'restructured', 'to', 'be', 'a', 'for', '-', 'profit', 'company.[35', ']', 'In', '2020', ',', 'Microsoft', 'announced', 'the', 'company', 'had', 'exclusive', 'licensing', 'of', 'GPT-3', 'for', 'Microsoft', \"'s\", 'products', 'and', 'services', 'following', 'a', 'multi', '-', 'billion', 'dollar', 'investment', 'in', 'OpenAI', '.', 'The', 'agreement', 'permits', 'OpenAI', 'to', 'offer', 'a', 'public', '-', 'facing', 'API', 'such', 'that', 'users', 'can', 'send', 'text', 'to', 'GPT-3', 'to', 'receive', 'the', 'model', \"'s\", 'output', ',', 'but', 'only', 'Microsoft', 'will', 'have', 'access', 'to', 'the', 'GPT-3', \"'s\", 'source', 'code.[6', ']', '\\n\\n', 'Large', 'language', 'models', ',', 'such', 'as', 'GPT-3', ',', 'have', 'come', 'under', 'criticism', 'from', 'Google', \"'s\", 'AI', 'ethics', 'researchers', 'for', 'the', 'environmental', 'impact', 'of', 'training', 'and', 'storing', 'the', 'models', ',', 'detailed', 'in', 'a', 'paper', 'co', '-', 'authored', 'by', 'Timnit', 'Gebru', 'and', 'Emily', 'M.', 'Bender', 'in', '2021.[36', ']', '\\n\\n', 'The', 'growing', 'use', 'of', 'automated', 'writing', 'technologies', 'based', 'on', 'GPT-3', 'and', 'other', 'language', 'generators', ',', 'has', 'raised', 'concerns', 'regarding', 'academic', 'integrity[37', ']', 'and', 'raised', 'the', 'stakes', 'of', 'how', 'universities', 'and', 'schools', 'will', 'gauge', 'what', 'constitutes', 'academic', 'misconduct', 'such', 'as', 'plagiarism.[38', ']', '\\n\\n', 'GPT-3', 'was', 'criticized', 'for', 'its', 'algorithmic', 'bias', ';', 'for', 'example', ',', 'it', 'is', 'more', 'likely', 'to', 'associate', 'Islam', 'with', 'terrorism.[39', ']', '\\n\\n', 'In', 'its', 'response', 'to', 'the', 'Request', 'for', 'Comments', 'on', 'Intellectual', 'Property', 'Protection', 'for', 'Artificial', 'Intelligence', 'Innovation', 'from', 'the', 'United', 'States', 'Patent', 'and', 'Trademark', 'Office', '(', '\"', 'USPTO', '\"', ')', ',', 'OpenAI', 'acknowledges', 'that', '\"', 'copyright', 'protection', 'arises', 'automatically', 'when', 'an', 'author', 'creates', 'an', 'original', 'work', 'and', 'fixes', 'it', 'in', 'a', 'tangible', 'medium', ',', 'see', '17', 'U.S.C.', '§', '102', ',', 'the', 'vast', 'majority', 'of', 'content', 'posted', 'online', 'is', 'protected', 'by', 'U.S.', 'copyright', 'laws.[40', ']', 'However', ',', 'GPT', 'was', 'built', 'with', 'data', 'from', 'the', 'Common', 'Crawl', 'dataset', ',', 'a', 'conglomerate', 'of', 'copyrighted', 'articles', ',', 'internet', 'posts', ',', 'web', 'pages', ',', 'and', 'books', 'scraped', 'from', '60', 'million', 'domains', 'over', 'a', 'period', 'of', '12', 'years', '.', 'TechCrunch', 'reports', 'this', 'data', 'includes', 'copyrighted', 'material', 'from', 'BBC', ',', 'The', 'New', 'York', 'Times', ',', 'Reddit', ',', 'the', 'full', 'text', 'of', 'online', 'books', ',', 'and', 'more.[41', ']', 'All', 'GPT', '-', 'based', 'products', 'therefore', 'run', 'the', 'risk', 'of', 'plagiarism', '.', 'In', 'April', '2021', ',', 'a', 'group', 'of', 'computer', 'scientists', 'used', 'a', 'tool', 'that', 'identifies', 'text', 'generated', 'by', 'GPT', 'in', 'an', 'effort', 'to', 'isolate', 'the', 'reason', 'for', 'strange', 'phrases', 'appearing', 'in', 'scientific', 'papers', '.', 'Cabanac', 'and', 'colleagues', 'ran', 'a', 'selection', 'of', 'abstracts', 'from', 'Microprocessors', 'and', 'Microsystems', 'through', 'this', 'tool', 'and', 'discovered', '\"', 'critical', 'flaws', '\"', ',', 'such', 'as', 'nonsensical', 'text', 'and', 'plagiarized', 'text', 'and', 'images.[42', ']', '\\n']\n"
     ]
    }
   ],
   "source": [
    "tokens=[token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b6d90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660736b",
   "metadata": {},
   "source": [
    "### Word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e7509dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq={}\n",
    "for word in doc:\n",
    "    if word.text.lower() not in stopwords:\n",
    "        if word.text.lower() not in punctuation:\n",
    "            if word.text.lower() not in '\\n':\n",
    "                if word.text not in word_freq.keys():\n",
    "                    word_freq[word.text]=1\n",
    "                else:\n",
    "                    word_freq[word.text]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a4fd137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Generative': 1,\n",
       " 'Pre': 1,\n",
       " 'trained': 6,\n",
       " 'Transformer': 2,\n",
       " '3': 4,\n",
       " 'GPT-3': 54,\n",
       " 'autoregressive': 1,\n",
       " 'language': 31,\n",
       " 'model': 18,\n",
       " 'uses': 1,\n",
       " 'deep': 3,\n",
       " 'learning': 6,\n",
       " 'produce': 1,\n",
       " 'human': 5,\n",
       " 'like': 1,\n",
       " 'text': 13,\n",
       " '\\n\\n': 16,\n",
       " 'generation': 3,\n",
       " 'prediction': 1,\n",
       " 'GPT': 6,\n",
       " 'n': 3,\n",
       " 'series': 2,\n",
       " 'successor': 1,\n",
       " 'GPT-2': 4,\n",
       " 'created': 1,\n",
       " 'OpenAI': 17,\n",
       " 'San': 1,\n",
       " 'Francisco': 1,\n",
       " 'based': 8,\n",
       " 'artificial': 2,\n",
       " 'intelligence': 2,\n",
       " 'research': 3,\n",
       " 'laboratory.[2': 1,\n",
       " 'version': 3,\n",
       " 'capacity': 4,\n",
       " '175': 1,\n",
       " 'billion': 13,\n",
       " 'machine': 3,\n",
       " 'parameters': 4,\n",
       " 'introduced': 3,\n",
       " '2020': 9,\n",
       " 'beta': 1,\n",
       " 'testing': 2,\n",
       " 'July': 2,\n",
       " '2020,[3': 1,\n",
       " 'trend': 1,\n",
       " 'natural': 4,\n",
       " 'processing': 5,\n",
       " 'NLP': 5,\n",
       " 'systems': 5,\n",
       " 'pre': 5,\n",
       " 'representations.[1': 1,\n",
       " 'quality': 1,\n",
       " 'generated': 3,\n",
       " 'high': 1,\n",
       " 'difficult': 1,\n",
       " 'determine': 1,\n",
       " 'written': 3,\n",
       " 'benefits': 1,\n",
       " 'risks.[4': 1,\n",
       " 'Thirty': 1,\n",
       " 'researchers': 5,\n",
       " 'engineers': 3,\n",
       " 'presented': 2,\n",
       " 'original': 4,\n",
       " '28': 3,\n",
       " 'paper': 5,\n",
       " 'introducing': 1,\n",
       " 'warned': 2,\n",
       " 'potential': 3,\n",
       " 'dangers': 2,\n",
       " 'called': 2,\n",
       " 'mitigate': 1,\n",
       " 'risk.[1': 1,\n",
       " '\\u200a': 7,\n",
       " '34': 3,\n",
       " '\\u200a ': 4,\n",
       " 'David': 2,\n",
       " 'Chalmers': 2,\n",
       " 'Australian': 2,\n",
       " 'philosopher': 2,\n",
       " 'described': 6,\n",
       " 'interesting': 2,\n",
       " 'important': 2,\n",
       " 'AI': 8,\n",
       " 'produced': 5,\n",
       " '\"[5': 1,\n",
       " 'Microsoft': 8,\n",
       " 'announced': 3,\n",
       " 'September': 1,\n",
       " '22': 3,\n",
       " 'licensed': 1,\n",
       " 'exclusive': 2,\n",
       " 'use': 5,\n",
       " 'public': 2,\n",
       " 'API': 5,\n",
       " 'receive': 2,\n",
       " 'output': 2,\n",
       " 'access': 4,\n",
       " 'underlying': 1,\n",
       " 'model.[6': 1,\n",
       " 'information': 2,\n",
       " '§': 2,\n",
       " 'Background': 1,\n",
       " 'According': 3,\n",
       " 'Economist': 1,\n",
       " 'improved': 2,\n",
       " 'algorithms': 1,\n",
       " 'powerful': 1,\n",
       " 'computers': 1,\n",
       " 'increase': 1,\n",
       " 'digitized': 1,\n",
       " 'data': 6,\n",
       " 'fueled': 1,\n",
       " 'revolution': 1,\n",
       " 'new': 2,\n",
       " 'techniques': 1,\n",
       " '2010s': 1,\n",
       " 'resulting': 1,\n",
       " 'rapid': 1,\n",
       " 'improvements': 1,\n",
       " 'tasks': 2,\n",
       " 'including': 1,\n",
       " 'manipulating': 1,\n",
       " 'language.[7': 1,\n",
       " 'Software': 1,\n",
       " 'models': 9,\n",
       " 'learn': 1,\n",
       " 'thousands': 1,\n",
       " 'millions': 1,\n",
       " 'examples': 1,\n",
       " 'structure': 1,\n",
       " '...': 2,\n",
       " 'loosely': 1,\n",
       " 'neural': 3,\n",
       " 'architecture': 3,\n",
       " 'brain\".[7': 1,\n",
       " 'network': 2,\n",
       " '2017': 1,\n",
       " '—': 5,\n",
       " 'Transformer.[8': 1,\n",
       " 'number': 2,\n",
       " 'capable': 3,\n",
       " 'mining': 1,\n",
       " 'organizing': 1,\n",
       " 'connecting': 1,\n",
       " 'contrasting': 1,\n",
       " 'understanding': 3,\n",
       " 'generating': 1,\n",
       " 'answers': 1,\n",
       " 'questions.[9': 1,\n",
       " 'June': 2,\n",
       " '11': 2,\n",
       " '2018': 1,\n",
       " 'posted': 2,\n",
       " 'generative': 3,\n",
       " 'enormous': 1,\n",
       " 'diverse': 2,\n",
       " 'corpus': 2,\n",
       " 'datasets': 1,\n",
       " 'process': 3,\n",
       " 'training': 8,\n",
       " 'GP).[10': 1,\n",
       " 'authors': 3,\n",
       " 'performances': 1,\n",
       " 'unlabeled': 1,\n",
       " 'followed': 1,\n",
       " 'discriminative': 1,\n",
       " 'fine': 1,\n",
       " 'tuning': 1,\n",
       " 'specific': 1,\n",
       " 'task': 2,\n",
       " 'eliminated': 1,\n",
       " 'need': 1,\n",
       " 'supervision': 1,\n",
       " 'time': 2,\n",
       " 'intensive': 1,\n",
       " 'hand': 1,\n",
       " 'labeling.[10': 1,\n",
       " 'February': 1,\n",
       " 'Turing': 2,\n",
       " 'Natural': 1,\n",
       " 'Language': 1,\n",
       " 'Generation': 1,\n",
       " 'T': 1,\n",
       " 'NLG': 2,\n",
       " 'claimed': 1,\n",
       " 'largest': 3,\n",
       " 'published': 1,\n",
       " '17': 2,\n",
       " '\"[11': 1,\n",
       " 'performed': 1,\n",
       " 'better': 2,\n",
       " 'variety': 1,\n",
       " 'included': 1,\n",
       " 'summarizing': 1,\n",
       " 'texts': 1,\n",
       " 'answering': 1,\n",
       " 'questions': 1,\n",
       " 'despite': 1,\n",
       " 'smaller': 1,\n",
       " 'IBM': 1,\n",
       " 'Tangora': 1,\n",
       " '8': 6,\n",
       " 'trillion': 1,\n",
       " 'parameters.[11': 1,\n",
       " 'Training': 3,\n",
       " 'capabilities[edit': 1,\n",
       " 'arXiv': 1,\n",
       " 'preprint': 1,\n",
       " 'group': 2,\n",
       " '31': 1,\n",
       " 'development': 1,\n",
       " 'state': 1,\n",
       " 'art': 1,\n",
       " 'model\".[1][4': 1,\n",
       " 'team': 1,\n",
       " 'increased': 2,\n",
       " 'orders': 1,\n",
       " 'magnitude': 1,\n",
       " 'predecessor': 2,\n",
       " 'GPT-2,[12': 1,\n",
       " 'making': 1,\n",
       " 'non': 2,\n",
       " 'sparse': 2,\n",
       " 'set': 1,\n",
       " 'constant': 1,\n",
       " 'value': 2,\n",
       " 'total': 2,\n",
       " 'meaningful': 1,\n",
       " 'date.[1': 1,\n",
       " '14': 1,\n",
       " '2': 1,\n",
       " 'structurally': 1,\n",
       " 'similar': 2,\n",
       " 'predecessors,[1': 1,\n",
       " 'higher': 3,\n",
       " 'level': 2,\n",
       " 'accuracy': 1,\n",
       " 'attributed': 1,\n",
       " 'parameters.[13': 1,\n",
       " 'times': 1,\n",
       " 'larger': 2,\n",
       " 'model.[4': 1,\n",
       " 'percent': 1,\n",
       " 'weighted': 2,\n",
       " 'dataset': 2,\n",
       " 'comes': 1,\n",
       " 'filtered': 1,\n",
       " 'Common': 3,\n",
       " 'Crawl': 3,\n",
       " 'consisting': 1,\n",
       " '410': 2,\n",
       " 'byte': 1,\n",
       " 'pair': 1,\n",
       " 'encoded': 1,\n",
       " 'tokens.[1': 1,\n",
       " '9': 2,\n",
       " 'sources': 1,\n",
       " '19': 2,\n",
       " 'tokens': 4,\n",
       " 'WebText2': 2,\n",
       " 'representing': 4,\n",
       " '12': 3,\n",
       " 'Books1': 2,\n",
       " '55': 2,\n",
       " 'Books2': 2,\n",
       " 'Wikipedia': 3,\n",
       " '3%.[1': 1,\n",
       " 'hundreds': 1,\n",
       " 'billions': 1,\n",
       " 'words': 2,\n",
       " 'coding': 1,\n",
       " 'CSS': 1,\n",
       " 'JSX': 1,\n",
       " 'Python': 1,\n",
       " 'others.[3': 1,\n",
       " 'Data': 1,\n",
       " 'Dataset': 1,\n",
       " '\\t': 12,\n",
       " 'Tokens': 1,\n",
       " 'Weight': 1,\n",
       " 'Mix': 1,\n",
       " '60': 2,\n",
       " 'encompassing': 1,\n",
       " 'require': 1,\n",
       " 'distinct': 1,\n",
       " 'tasks.[3': 1,\n",
       " 'contains': 1,\n",
       " 'occasional': 1,\n",
       " 'toxic': 5,\n",
       " 'occasionally': 1,\n",
       " 'generates': 2,\n",
       " 'result': 1,\n",
       " 'mimicking': 1,\n",
       " 'study': 1,\n",
       " 'University': 1,\n",
       " 'Washington': 1,\n",
       " 'found': 1,\n",
       " 'toxicity': 2,\n",
       " 'comparable': 1,\n",
       " 'CTRL': 2,\n",
       " 'compared': 2,\n",
       " 'GPT-1': 1,\n",
       " 'generations': 1,\n",
       " 'Wiki': 1,\n",
       " 'entirely': 1,\n",
       " 'data.[14': 1,\n",
       " 'users': 3,\n",
       " 'request': 1,\n",
       " 'user': 2,\n",
       " 'friendly': 1,\n",
       " 'toolset\"—to': 1,\n",
       " 'help': 1,\n",
       " 'explore': 1,\n",
       " 'strengths': 1,\n",
       " 'limits': 1,\n",
       " 'technology.[15][16': 1,\n",
       " 'invitation': 1,\n",
       " 'general': 2,\n",
       " 'purpose': 2,\n",
       " 'interface': 1,\n",
       " 'complete': 1,\n",
       " 'English': 1,\n",
       " 'instead': 1,\n",
       " 'usual': 1,\n",
       " 'single': 1,\n",
       " 'case.[15': 1,\n",
       " 'private': 1,\n",
       " 'early': 1,\n",
       " 'release': 2,\n",
       " 'eerily': 1,\n",
       " 'good': 2,\n",
       " 'writing': 3,\n",
       " 'amazingly': 1,\n",
       " 'coherent': 1,\n",
       " 'simple': 1,\n",
       " 'prompts.[17': 1,\n",
       " 'initial': 1,\n",
       " 'experiment': 1,\n",
       " '80': 1,\n",
       " 'subjects': 1,\n",
       " 'asked': 2,\n",
       " 'judge': 1,\n",
       " 'short': 1,\n",
       " '~200': 1,\n",
       " 'word': 2,\n",
       " 'articles': 5,\n",
       " 'humans': 1,\n",
       " 'participants': 1,\n",
       " 'judged': 1,\n",
       " 'incorrectly': 1,\n",
       " '48': 1,\n",
       " 'slightly': 1,\n",
       " 'random': 1,\n",
       " 'guessing.[1': 1,\n",
       " 'generate': 2,\n",
       " 'news': 2,\n",
       " 'evaluators': 1,\n",
       " 'difficulty': 1,\n",
       " 'distinguishing': 1,\n",
       " 'humans,\"[4': 1,\n",
       " 'advance': 1,\n",
       " 'beneficial': 1,\n",
       " 'harmful': 2,\n",
       " 'applications': 2,\n",
       " '\"[1': 1,\n",
       " 'detail': 1,\n",
       " 'effects': 1,\n",
       " 'GPT-3\"[4': 1,\n",
       " 'include': 1,\n",
       " 'misinformation': 1,\n",
       " 'spam': 1,\n",
       " 'phishing': 1,\n",
       " 'abuse': 1,\n",
       " 'legal': 1,\n",
       " 'governmental': 1,\n",
       " 'processes': 1,\n",
       " 'fraudulent': 1,\n",
       " 'academic': 3,\n",
       " 'essay': 1,\n",
       " 'social': 1,\n",
       " 'engineering': 1,\n",
       " 'pretexting\".[1': 1,\n",
       " 'draw': 1,\n",
       " 'attention': 1,\n",
       " 'risk': 3,\n",
       " 'mitigation.[1][18': 1,\n",
       " '\\u200a\\n\\n': 1,\n",
       " 'performing': 1,\n",
       " 'zero': 1,\n",
       " 'shot': 3,\n",
       " 'learning.[1': 1,\n",
       " 'Reception[edit': 1,\n",
       " 'Applications[edit': 1,\n",
       " 'specifically': 1,\n",
       " 'Codex': 1,\n",
       " 'basis': 1,\n",
       " 'GitHub': 1,\n",
       " 'Copilot': 1,\n",
       " 'code': 3,\n",
       " 'completion': 1,\n",
       " 'software': 1,\n",
       " 'editors': 1,\n",
       " 'IDEs': 1,\n",
       " 'certain': 1,\n",
       " 'products': 3,\n",
       " 'translate': 1,\n",
       " 'conventional': 1,\n",
       " 'formal': 1,\n",
       " 'computer': 3,\n",
       " 'code.[19': 1,\n",
       " 'Andrew': 1,\n",
       " 'Mayne': 1,\n",
       " 'Writer,[20': 1,\n",
       " 'allows': 2,\n",
       " 'people': 2,\n",
       " 'correspond': 1,\n",
       " 'historical': 1,\n",
       " 'figures': 1,\n",
       " 'email': 1,\n",
       " 'Jason': 1,\n",
       " 'Rohrer': 1,\n",
       " 'retro': 1,\n",
       " 'themed': 1,\n",
       " 'chatbot': 2,\n",
       " 'project': 1,\n",
       " 'named': 1,\n",
       " 'Project': 1,\n",
       " 'December': 1,\n",
       " 'accessible': 1,\n",
       " 'online': 3,\n",
       " 'converse': 1,\n",
       " 'AIs': 1,\n",
       " 'technology.[21': 1,\n",
       " 'Guardian': 1,\n",
       " 'write': 1,\n",
       " 'article': 2,\n",
       " 'harmless': 1,\n",
       " 'beings': 1,\n",
       " 'fed': 1,\n",
       " 'ideas': 1,\n",
       " 'different': 1,\n",
       " 'essays': 1,\n",
       " 'ultimately': 1,\n",
       " 'merged': 1,\n",
       " 'article.[22': 1,\n",
       " 'Dungeon': 1,\n",
       " 'adventure': 1,\n",
       " 'games': 1,\n",
       " 'Reviews[edit': 1,\n",
       " 'review': 2,\n",
       " 'New': 2,\n",
       " 'York': 2,\n",
       " 'Times': 2,\n",
       " 'Farhad': 1,\n",
       " 'Manjoo': 1,\n",
       " 'said': 4,\n",
       " 'ability': 1,\n",
       " 'poetry': 1,\n",
       " 'prose': 1,\n",
       " 'amazing': 1,\n",
       " 'spooky': 1,\n",
       " 'humbling': 1,\n",
       " 'little': 1,\n",
       " 'terrifying\".[23': 1,\n",
       " 'Daily': 1,\n",
       " 'Nous': 1,\n",
       " 'philosophers': 1,\n",
       " 'GPT-3.[24': 1,\n",
       " 'produced\".[5': 1,\n",
       " 'Wired': 1,\n",
       " 'provoking': 1,\n",
       " 'chills': 1,\n",
       " 'Silicon': 1,\n",
       " 'Valley\".[25': 1,\n",
       " 'National': 1,\n",
       " 'Law': 1,\n",
       " 'Review': 2,\n",
       " 'impressive': 1,\n",
       " 'step': 1,\n",
       " 'finding': 1,\n",
       " 'useful': 2,\n",
       " 'power': 1,\n",
       " 'continuing': 1,\n",
       " 'work': 2,\n",
       " 'intelligence\".[26': 1,\n",
       " 'MIT': 1,\n",
       " 'Technology': 1,\n",
       " 'cowritten': 1,\n",
       " 'Deep': 1,\n",
       " 'Learning': 1,\n",
       " 'critic': 1,\n",
       " 'Gary': 1,\n",
       " 'Marcus,[27': 1,\n",
       " 'stated': 1,\n",
       " 'comprehension': 1,\n",
       " 'world': 1,\n",
       " 'seriously': 1,\n",
       " 'means': 1,\n",
       " 'trust': 1,\n",
       " 'says': 1,\n",
       " '\"[28': 1,\n",
       " 'relationships': 1,\n",
       " 'having': 1,\n",
       " 'meaning': 1,\n",
       " 'Jerome': 1,\n",
       " 'Pesenti': 1,\n",
       " 'head': 1,\n",
       " 'Facebook': 1,\n",
       " 'lab': 1,\n",
       " 'unsafe': 1,\n",
       " 'pointing': 1,\n",
       " 'sexist': 1,\n",
       " 'racist': 1,\n",
       " 'biased': 1,\n",
       " 'negative': 1,\n",
       " 'system': 1,\n",
       " 'discuss': 1,\n",
       " 'Jews': 1,\n",
       " 'women': 1,\n",
       " 'black': 1,\n",
       " 'Holocaust.[29': 1,\n",
       " 'Nabla': 1,\n",
       " 'French': 1,\n",
       " 'start': 1,\n",
       " 'specializing': 1,\n",
       " 'healthcare': 1,\n",
       " 'technology': 1,\n",
       " 'tested': 1,\n",
       " 'medical': 1,\n",
       " 'expected': 1,\n",
       " 'showed': 1,\n",
       " 'limitations': 1,\n",
       " 'example': 2,\n",
       " 'responses': 1,\n",
       " 'mental': 1,\n",
       " 'health': 1,\n",
       " 'issues': 1,\n",
       " 'advised': 1,\n",
       " 'simulated': 1,\n",
       " 'patient': 1,\n",
       " 'commit': 1,\n",
       " 'suicide.[30': 1,\n",
       " 'Noam': 1,\n",
       " 'Chomsky': 1,\n",
       " 'expressed': 1,\n",
       " 'skepticism': 1,\n",
       " 'scientific': 3,\n",
       " 'works': 1,\n",
       " 'impossible': 1,\n",
       " 'languages': 2,\n",
       " 'actual': 1,\n",
       " 'refuted': 1,\n",
       " 'intended': 1,\n",
       " 'normal': 1,\n",
       " 'criteria': 1,\n",
       " 'tell': 1,\n",
       " 'cognition': 1,\n",
       " 'generally': 1,\n",
       " '\"[31': 1,\n",
       " 'Luciano': 1,\n",
       " 'Floridi': 1,\n",
       " 'Massimo': 1,\n",
       " 'Chiriatti': 1,\n",
       " 'highlighted': 1,\n",
       " 'cheap': 1,\n",
       " 'production': 1,\n",
       " 'semantic': 1,\n",
       " 'artefacts\".[32': 1,\n",
       " 'Criticism[edit': 1,\n",
       " 'builder': 1,\n",
       " 'initially': 1,\n",
       " 'founded': 1,\n",
       " 'profit': 2,\n",
       " '2015.[33': 1,\n",
       " '2019': 1,\n",
       " 'publicly': 1,\n",
       " 'precursor': 1,\n",
       " 'breaking': 1,\n",
       " 'previous': 1,\n",
       " 'open': 1,\n",
       " 'source': 2,\n",
       " 'practices': 1,\n",
       " 'citing': 1,\n",
       " 'concerns': 2,\n",
       " 'perpetuate': 1,\n",
       " 'fake': 1,\n",
       " 'eventually': 1,\n",
       " 'released': 1,\n",
       " 'size.[34': 1,\n",
       " 'year': 1,\n",
       " 'restructured': 1,\n",
       " 'company.[35': 1,\n",
       " 'company': 1,\n",
       " 'licensing': 1,\n",
       " 'services': 1,\n",
       " 'following': 1,\n",
       " 'multi': 1,\n",
       " 'dollar': 1,\n",
       " 'investment': 1,\n",
       " 'agreement': 1,\n",
       " 'permits': 1,\n",
       " 'offer': 1,\n",
       " 'facing': 1,\n",
       " 'send': 1,\n",
       " 'code.[6': 1,\n",
       " 'Large': 1,\n",
       " 'come': 1,\n",
       " 'criticism': 1,\n",
       " 'Google': 1,\n",
       " 'ethics': 1,\n",
       " 'environmental': 1,\n",
       " 'impact': 1,\n",
       " 'storing': 1,\n",
       " 'detailed': 1,\n",
       " 'co': 1,\n",
       " 'authored': 1,\n",
       " 'Timnit': 1,\n",
       " 'Gebru': 1,\n",
       " 'Emily': 1,\n",
       " 'M.': 1,\n",
       " 'Bender': 1,\n",
       " '2021.[36': 1,\n",
       " 'growing': 1,\n",
       " 'automated': 1,\n",
       " 'technologies': 1,\n",
       " 'generators': 1,\n",
       " 'raised': 2,\n",
       " 'integrity[37': 1,\n",
       " 'stakes': 1,\n",
       " 'universities': 1,\n",
       " 'schools': 1,\n",
       " 'gauge': 1,\n",
       " 'constitutes': 1,\n",
       " 'misconduct': 1,\n",
       " 'plagiarism.[38': 1,\n",
       " 'criticized': 1,\n",
       " 'algorithmic': 1,\n",
       " 'bias': 1,\n",
       " 'likely': 1,\n",
       " 'associate': 1,\n",
       " 'Islam': 1,\n",
       " 'terrorism.[39': 1,\n",
       " 'response': 1,\n",
       " 'Request': 1,\n",
       " 'Comments': 1,\n",
       " 'Intellectual': 1,\n",
       " 'Property': 1,\n",
       " 'Protection': 1,\n",
       " 'Artificial': 1,\n",
       " 'Intelligence': 1,\n",
       " 'Innovation': 1,\n",
       " 'United': 1,\n",
       " 'States': 1,\n",
       " 'Patent': 1,\n",
       " 'Trademark': 1,\n",
       " 'Office': 1,\n",
       " 'USPTO': 1,\n",
       " 'acknowledges': 1,\n",
       " 'copyright': 2,\n",
       " 'protection': 1,\n",
       " 'arises': 1,\n",
       " 'automatically': 1,\n",
       " 'author': 1,\n",
       " 'creates': 1,\n",
       " 'fixes': 1,\n",
       " 'tangible': 1,\n",
       " 'medium': 1,\n",
       " 'U.S.C.': 1,\n",
       " '102': 1,\n",
       " 'vast': 1,\n",
       " 'majority': 1,\n",
       " 'content': 1,\n",
       " 'protected': 1,\n",
       " 'U.S.': 1,\n",
       " 'laws.[40': 1,\n",
       " 'built': 1,\n",
       " 'conglomerate': 1,\n",
       " 'copyrighted': 2,\n",
       " 'internet': 1,\n",
       " 'posts': 1,\n",
       " 'web': 1,\n",
       " 'pages': 1,\n",
       " 'books': 2,\n",
       " 'scraped': 1,\n",
       " 'million': 1,\n",
       " 'domains': 1,\n",
       " 'period': 1,\n",
       " 'years': 1,\n",
       " 'TechCrunch': 1,\n",
       " 'reports': 1,\n",
       " 'includes': 1,\n",
       " 'material': 1,\n",
       " 'BBC': 1,\n",
       " 'Reddit': 1,\n",
       " 'more.[41': 1,\n",
       " 'run': 1,\n",
       " 'plagiarism': 1,\n",
       " 'April': 1,\n",
       " '2021': 1,\n",
       " 'scientists': 1,\n",
       " 'tool': 2,\n",
       " 'identifies': 1,\n",
       " 'effort': 1,\n",
       " 'isolate': 1,\n",
       " 'reason': 1,\n",
       " 'strange': 1,\n",
       " 'phrases': 1,\n",
       " 'appearing': 1,\n",
       " 'papers': 1,\n",
       " 'Cabanac': 1,\n",
       " 'colleagues': 1,\n",
       " 'ran': 1,\n",
       " 'selection': 1,\n",
       " 'abstracts': 1,\n",
       " 'Microprocessors': 1,\n",
       " 'Microsystems': 1,\n",
       " 'discovered': 1,\n",
       " 'critical': 1,\n",
       " 'flaws': 1,\n",
       " 'nonsensical': 1,\n",
       " 'plagiarized': 1,\n",
       " 'images.[42': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96d43e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "max_freq=max(word_freq.values())\n",
    "print(max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad874e9d",
   "metadata": {},
   "source": [
    "### Calculating most frequently occured word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d436a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in word_freq.keys():\n",
    "    word_freq[word]=word_freq[word]/max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f1b8591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Generative': 0.018518518518518517,\n",
       " 'Pre': 0.018518518518518517,\n",
       " 'trained': 0.1111111111111111,\n",
       " 'Transformer': 0.037037037037037035,\n",
       " '3': 0.07407407407407407,\n",
       " 'GPT-3': 1.0,\n",
       " 'autoregressive': 0.018518518518518517,\n",
       " 'language': 0.5740740740740741,\n",
       " 'model': 0.3333333333333333,\n",
       " 'uses': 0.018518518518518517,\n",
       " 'deep': 0.05555555555555555,\n",
       " 'learning': 0.1111111111111111,\n",
       " 'produce': 0.018518518518518517,\n",
       " 'human': 0.09259259259259259,\n",
       " 'like': 0.018518518518518517,\n",
       " 'text': 0.24074074074074073,\n",
       " '\\n\\n': 0.2962962962962963,\n",
       " 'generation': 0.05555555555555555,\n",
       " 'prediction': 0.018518518518518517,\n",
       " 'GPT': 0.1111111111111111,\n",
       " 'n': 0.05555555555555555,\n",
       " 'series': 0.037037037037037035,\n",
       " 'successor': 0.018518518518518517,\n",
       " 'GPT-2': 0.07407407407407407,\n",
       " 'created': 0.018518518518518517,\n",
       " 'OpenAI': 0.3148148148148148,\n",
       " 'San': 0.018518518518518517,\n",
       " 'Francisco': 0.018518518518518517,\n",
       " 'based': 0.14814814814814814,\n",
       " 'artificial': 0.037037037037037035,\n",
       " 'intelligence': 0.037037037037037035,\n",
       " 'research': 0.05555555555555555,\n",
       " 'laboratory.[2': 0.018518518518518517,\n",
       " 'version': 0.05555555555555555,\n",
       " 'capacity': 0.07407407407407407,\n",
       " '175': 0.018518518518518517,\n",
       " 'billion': 0.24074074074074073,\n",
       " 'machine': 0.05555555555555555,\n",
       " 'parameters': 0.07407407407407407,\n",
       " 'introduced': 0.05555555555555555,\n",
       " '2020': 0.16666666666666666,\n",
       " 'beta': 0.018518518518518517,\n",
       " 'testing': 0.037037037037037035,\n",
       " 'July': 0.037037037037037035,\n",
       " '2020,[3': 0.018518518518518517,\n",
       " 'trend': 0.018518518518518517,\n",
       " 'natural': 0.07407407407407407,\n",
       " 'processing': 0.09259259259259259,\n",
       " 'NLP': 0.09259259259259259,\n",
       " 'systems': 0.09259259259259259,\n",
       " 'pre': 0.09259259259259259,\n",
       " 'representations.[1': 0.018518518518518517,\n",
       " 'quality': 0.018518518518518517,\n",
       " 'generated': 0.05555555555555555,\n",
       " 'high': 0.018518518518518517,\n",
       " 'difficult': 0.018518518518518517,\n",
       " 'determine': 0.018518518518518517,\n",
       " 'written': 0.05555555555555555,\n",
       " 'benefits': 0.018518518518518517,\n",
       " 'risks.[4': 0.018518518518518517,\n",
       " 'Thirty': 0.018518518518518517,\n",
       " 'researchers': 0.09259259259259259,\n",
       " 'engineers': 0.05555555555555555,\n",
       " 'presented': 0.037037037037037035,\n",
       " 'original': 0.07407407407407407,\n",
       " '28': 0.05555555555555555,\n",
       " 'paper': 0.09259259259259259,\n",
       " 'introducing': 0.018518518518518517,\n",
       " 'warned': 0.037037037037037035,\n",
       " 'potential': 0.05555555555555555,\n",
       " 'dangers': 0.037037037037037035,\n",
       " 'called': 0.037037037037037035,\n",
       " 'mitigate': 0.018518518518518517,\n",
       " 'risk.[1': 0.018518518518518517,\n",
       " '\\u200a': 0.12962962962962962,\n",
       " '34': 0.05555555555555555,\n",
       " '\\u200a ': 0.07407407407407407,\n",
       " 'David': 0.037037037037037035,\n",
       " 'Chalmers': 0.037037037037037035,\n",
       " 'Australian': 0.037037037037037035,\n",
       " 'philosopher': 0.037037037037037035,\n",
       " 'described': 0.1111111111111111,\n",
       " 'interesting': 0.037037037037037035,\n",
       " 'important': 0.037037037037037035,\n",
       " 'AI': 0.14814814814814814,\n",
       " 'produced': 0.09259259259259259,\n",
       " '\"[5': 0.018518518518518517,\n",
       " 'Microsoft': 0.14814814814814814,\n",
       " 'announced': 0.05555555555555555,\n",
       " 'September': 0.018518518518518517,\n",
       " '22': 0.05555555555555555,\n",
       " 'licensed': 0.018518518518518517,\n",
       " 'exclusive': 0.037037037037037035,\n",
       " 'use': 0.09259259259259259,\n",
       " 'public': 0.037037037037037035,\n",
       " 'API': 0.09259259259259259,\n",
       " 'receive': 0.037037037037037035,\n",
       " 'output': 0.037037037037037035,\n",
       " 'access': 0.07407407407407407,\n",
       " 'underlying': 0.018518518518518517,\n",
       " 'model.[6': 0.018518518518518517,\n",
       " 'information': 0.037037037037037035,\n",
       " '§': 0.037037037037037035,\n",
       " 'Background': 0.018518518518518517,\n",
       " 'According': 0.05555555555555555,\n",
       " 'Economist': 0.018518518518518517,\n",
       " 'improved': 0.037037037037037035,\n",
       " 'algorithms': 0.018518518518518517,\n",
       " 'powerful': 0.018518518518518517,\n",
       " 'computers': 0.018518518518518517,\n",
       " 'increase': 0.018518518518518517,\n",
       " 'digitized': 0.018518518518518517,\n",
       " 'data': 0.1111111111111111,\n",
       " 'fueled': 0.018518518518518517,\n",
       " 'revolution': 0.018518518518518517,\n",
       " 'new': 0.037037037037037035,\n",
       " 'techniques': 0.018518518518518517,\n",
       " '2010s': 0.018518518518518517,\n",
       " 'resulting': 0.018518518518518517,\n",
       " 'rapid': 0.018518518518518517,\n",
       " 'improvements': 0.018518518518518517,\n",
       " 'tasks': 0.037037037037037035,\n",
       " 'including': 0.018518518518518517,\n",
       " 'manipulating': 0.018518518518518517,\n",
       " 'language.[7': 0.018518518518518517,\n",
       " 'Software': 0.018518518518518517,\n",
       " 'models': 0.16666666666666666,\n",
       " 'learn': 0.018518518518518517,\n",
       " 'thousands': 0.018518518518518517,\n",
       " 'millions': 0.018518518518518517,\n",
       " 'examples': 0.018518518518518517,\n",
       " 'structure': 0.018518518518518517,\n",
       " '...': 0.037037037037037035,\n",
       " 'loosely': 0.018518518518518517,\n",
       " 'neural': 0.05555555555555555,\n",
       " 'architecture': 0.05555555555555555,\n",
       " 'brain\".[7': 0.018518518518518517,\n",
       " 'network': 0.037037037037037035,\n",
       " '2017': 0.018518518518518517,\n",
       " '—': 0.09259259259259259,\n",
       " 'Transformer.[8': 0.018518518518518517,\n",
       " 'number': 0.037037037037037035,\n",
       " 'capable': 0.05555555555555555,\n",
       " 'mining': 0.018518518518518517,\n",
       " 'organizing': 0.018518518518518517,\n",
       " 'connecting': 0.018518518518518517,\n",
       " 'contrasting': 0.018518518518518517,\n",
       " 'understanding': 0.05555555555555555,\n",
       " 'generating': 0.018518518518518517,\n",
       " 'answers': 0.018518518518518517,\n",
       " 'questions.[9': 0.018518518518518517,\n",
       " 'June': 0.037037037037037035,\n",
       " '11': 0.037037037037037035,\n",
       " '2018': 0.018518518518518517,\n",
       " 'posted': 0.037037037037037035,\n",
       " 'generative': 0.05555555555555555,\n",
       " 'enormous': 0.018518518518518517,\n",
       " 'diverse': 0.037037037037037035,\n",
       " 'corpus': 0.037037037037037035,\n",
       " 'datasets': 0.018518518518518517,\n",
       " 'process': 0.05555555555555555,\n",
       " 'training': 0.14814814814814814,\n",
       " 'GP).[10': 0.018518518518518517,\n",
       " 'authors': 0.05555555555555555,\n",
       " 'performances': 0.018518518518518517,\n",
       " 'unlabeled': 0.018518518518518517,\n",
       " 'followed': 0.018518518518518517,\n",
       " 'discriminative': 0.018518518518518517,\n",
       " 'fine': 0.018518518518518517,\n",
       " 'tuning': 0.018518518518518517,\n",
       " 'specific': 0.018518518518518517,\n",
       " 'task': 0.037037037037037035,\n",
       " 'eliminated': 0.018518518518518517,\n",
       " 'need': 0.018518518518518517,\n",
       " 'supervision': 0.018518518518518517,\n",
       " 'time': 0.037037037037037035,\n",
       " 'intensive': 0.018518518518518517,\n",
       " 'hand': 0.018518518518518517,\n",
       " 'labeling.[10': 0.018518518518518517,\n",
       " 'February': 0.018518518518518517,\n",
       " 'Turing': 0.037037037037037035,\n",
       " 'Natural': 0.018518518518518517,\n",
       " 'Language': 0.018518518518518517,\n",
       " 'Generation': 0.018518518518518517,\n",
       " 'T': 0.018518518518518517,\n",
       " 'NLG': 0.037037037037037035,\n",
       " 'claimed': 0.018518518518518517,\n",
       " 'largest': 0.05555555555555555,\n",
       " 'published': 0.018518518518518517,\n",
       " '17': 0.037037037037037035,\n",
       " '\"[11': 0.018518518518518517,\n",
       " 'performed': 0.018518518518518517,\n",
       " 'better': 0.037037037037037035,\n",
       " 'variety': 0.018518518518518517,\n",
       " 'included': 0.018518518518518517,\n",
       " 'summarizing': 0.018518518518518517,\n",
       " 'texts': 0.018518518518518517,\n",
       " 'answering': 0.018518518518518517,\n",
       " 'questions': 0.018518518518518517,\n",
       " 'despite': 0.018518518518518517,\n",
       " 'smaller': 0.018518518518518517,\n",
       " 'IBM': 0.018518518518518517,\n",
       " 'Tangora': 0.018518518518518517,\n",
       " '8': 0.1111111111111111,\n",
       " 'trillion': 0.018518518518518517,\n",
       " 'parameters.[11': 0.018518518518518517,\n",
       " 'Training': 0.05555555555555555,\n",
       " 'capabilities[edit': 0.018518518518518517,\n",
       " 'arXiv': 0.018518518518518517,\n",
       " 'preprint': 0.018518518518518517,\n",
       " 'group': 0.037037037037037035,\n",
       " '31': 0.018518518518518517,\n",
       " 'development': 0.018518518518518517,\n",
       " 'state': 0.018518518518518517,\n",
       " 'art': 0.018518518518518517,\n",
       " 'model\".[1][4': 0.018518518518518517,\n",
       " 'team': 0.018518518518518517,\n",
       " 'increased': 0.037037037037037035,\n",
       " 'orders': 0.018518518518518517,\n",
       " 'magnitude': 0.018518518518518517,\n",
       " 'predecessor': 0.037037037037037035,\n",
       " 'GPT-2,[12': 0.018518518518518517,\n",
       " 'making': 0.018518518518518517,\n",
       " 'non': 0.037037037037037035,\n",
       " 'sparse': 0.037037037037037035,\n",
       " 'set': 0.018518518518518517,\n",
       " 'constant': 0.018518518518518517,\n",
       " 'value': 0.037037037037037035,\n",
       " 'total': 0.037037037037037035,\n",
       " 'meaningful': 0.018518518518518517,\n",
       " 'date.[1': 0.018518518518518517,\n",
       " '14': 0.018518518518518517,\n",
       " '2': 0.018518518518518517,\n",
       " 'structurally': 0.018518518518518517,\n",
       " 'similar': 0.037037037037037035,\n",
       " 'predecessors,[1': 0.018518518518518517,\n",
       " 'higher': 0.05555555555555555,\n",
       " 'level': 0.037037037037037035,\n",
       " 'accuracy': 0.018518518518518517,\n",
       " 'attributed': 0.018518518518518517,\n",
       " 'parameters.[13': 0.018518518518518517,\n",
       " 'times': 0.018518518518518517,\n",
       " 'larger': 0.037037037037037035,\n",
       " 'model.[4': 0.018518518518518517,\n",
       " 'percent': 0.018518518518518517,\n",
       " 'weighted': 0.037037037037037035,\n",
       " 'dataset': 0.037037037037037035,\n",
       " 'comes': 0.018518518518518517,\n",
       " 'filtered': 0.018518518518518517,\n",
       " 'Common': 0.05555555555555555,\n",
       " 'Crawl': 0.05555555555555555,\n",
       " 'consisting': 0.018518518518518517,\n",
       " '410': 0.037037037037037035,\n",
       " 'byte': 0.018518518518518517,\n",
       " 'pair': 0.018518518518518517,\n",
       " 'encoded': 0.018518518518518517,\n",
       " 'tokens.[1': 0.018518518518518517,\n",
       " '9': 0.037037037037037035,\n",
       " 'sources': 0.018518518518518517,\n",
       " '19': 0.037037037037037035,\n",
       " 'tokens': 0.07407407407407407,\n",
       " 'WebText2': 0.037037037037037035,\n",
       " 'representing': 0.07407407407407407,\n",
       " '12': 0.05555555555555555,\n",
       " 'Books1': 0.037037037037037035,\n",
       " '55': 0.037037037037037035,\n",
       " 'Books2': 0.037037037037037035,\n",
       " 'Wikipedia': 0.05555555555555555,\n",
       " '3%.[1': 0.018518518518518517,\n",
       " 'hundreds': 0.018518518518518517,\n",
       " 'billions': 0.018518518518518517,\n",
       " 'words': 0.037037037037037035,\n",
       " 'coding': 0.018518518518518517,\n",
       " 'CSS': 0.018518518518518517,\n",
       " 'JSX': 0.018518518518518517,\n",
       " 'Python': 0.018518518518518517,\n",
       " 'others.[3': 0.018518518518518517,\n",
       " 'Data': 0.018518518518518517,\n",
       " 'Dataset': 0.018518518518518517,\n",
       " '\\t': 0.2222222222222222,\n",
       " 'Tokens': 0.018518518518518517,\n",
       " 'Weight': 0.018518518518518517,\n",
       " 'Mix': 0.018518518518518517,\n",
       " '60': 0.037037037037037035,\n",
       " 'encompassing': 0.018518518518518517,\n",
       " 'require': 0.018518518518518517,\n",
       " 'distinct': 0.018518518518518517,\n",
       " 'tasks.[3': 0.018518518518518517,\n",
       " 'contains': 0.018518518518518517,\n",
       " 'occasional': 0.018518518518518517,\n",
       " 'toxic': 0.09259259259259259,\n",
       " 'occasionally': 0.018518518518518517,\n",
       " 'generates': 0.037037037037037035,\n",
       " 'result': 0.018518518518518517,\n",
       " 'mimicking': 0.018518518518518517,\n",
       " 'study': 0.018518518518518517,\n",
       " 'University': 0.018518518518518517,\n",
       " 'Washington': 0.018518518518518517,\n",
       " 'found': 0.018518518518518517,\n",
       " 'toxicity': 0.037037037037037035,\n",
       " 'comparable': 0.018518518518518517,\n",
       " 'CTRL': 0.037037037037037035,\n",
       " 'compared': 0.037037037037037035,\n",
       " 'GPT-1': 0.018518518518518517,\n",
       " 'generations': 0.018518518518518517,\n",
       " 'Wiki': 0.018518518518518517,\n",
       " 'entirely': 0.018518518518518517,\n",
       " 'data.[14': 0.018518518518518517,\n",
       " 'users': 0.05555555555555555,\n",
       " 'request': 0.018518518518518517,\n",
       " 'user': 0.037037037037037035,\n",
       " 'friendly': 0.018518518518518517,\n",
       " 'toolset\"—to': 0.018518518518518517,\n",
       " 'help': 0.018518518518518517,\n",
       " 'explore': 0.018518518518518517,\n",
       " 'strengths': 0.018518518518518517,\n",
       " 'limits': 0.018518518518518517,\n",
       " 'technology.[15][16': 0.018518518518518517,\n",
       " 'invitation': 0.018518518518518517,\n",
       " 'general': 0.037037037037037035,\n",
       " 'purpose': 0.037037037037037035,\n",
       " 'interface': 0.018518518518518517,\n",
       " 'complete': 0.018518518518518517,\n",
       " 'English': 0.018518518518518517,\n",
       " 'instead': 0.018518518518518517,\n",
       " 'usual': 0.018518518518518517,\n",
       " 'single': 0.018518518518518517,\n",
       " 'case.[15': 0.018518518518518517,\n",
       " 'private': 0.018518518518518517,\n",
       " 'early': 0.018518518518518517,\n",
       " 'release': 0.037037037037037035,\n",
       " 'eerily': 0.018518518518518517,\n",
       " 'good': 0.037037037037037035,\n",
       " 'writing': 0.05555555555555555,\n",
       " 'amazingly': 0.018518518518518517,\n",
       " 'coherent': 0.018518518518518517,\n",
       " 'simple': 0.018518518518518517,\n",
       " 'prompts.[17': 0.018518518518518517,\n",
       " 'initial': 0.018518518518518517,\n",
       " 'experiment': 0.018518518518518517,\n",
       " '80': 0.018518518518518517,\n",
       " 'subjects': 0.018518518518518517,\n",
       " 'asked': 0.037037037037037035,\n",
       " 'judge': 0.018518518518518517,\n",
       " 'short': 0.018518518518518517,\n",
       " '~200': 0.018518518518518517,\n",
       " 'word': 0.037037037037037035,\n",
       " 'articles': 0.09259259259259259,\n",
       " 'humans': 0.018518518518518517,\n",
       " 'participants': 0.018518518518518517,\n",
       " 'judged': 0.018518518518518517,\n",
       " 'incorrectly': 0.018518518518518517,\n",
       " '48': 0.018518518518518517,\n",
       " 'slightly': 0.018518518518518517,\n",
       " 'random': 0.018518518518518517,\n",
       " 'guessing.[1': 0.018518518518518517,\n",
       " 'generate': 0.037037037037037035,\n",
       " 'news': 0.037037037037037035,\n",
       " 'evaluators': 0.018518518518518517,\n",
       " 'difficulty': 0.018518518518518517,\n",
       " 'distinguishing': 0.018518518518518517,\n",
       " 'humans,\"[4': 0.018518518518518517,\n",
       " 'advance': 0.018518518518518517,\n",
       " 'beneficial': 0.018518518518518517,\n",
       " 'harmful': 0.037037037037037035,\n",
       " 'applications': 0.037037037037037035,\n",
       " '\"[1': 0.018518518518518517,\n",
       " 'detail': 0.018518518518518517,\n",
       " 'effects': 0.018518518518518517,\n",
       " 'GPT-3\"[4': 0.018518518518518517,\n",
       " 'include': 0.018518518518518517,\n",
       " 'misinformation': 0.018518518518518517,\n",
       " 'spam': 0.018518518518518517,\n",
       " 'phishing': 0.018518518518518517,\n",
       " 'abuse': 0.018518518518518517,\n",
       " 'legal': 0.018518518518518517,\n",
       " 'governmental': 0.018518518518518517,\n",
       " 'processes': 0.018518518518518517,\n",
       " 'fraudulent': 0.018518518518518517,\n",
       " 'academic': 0.05555555555555555,\n",
       " 'essay': 0.018518518518518517,\n",
       " 'social': 0.018518518518518517,\n",
       " 'engineering': 0.018518518518518517,\n",
       " 'pretexting\".[1': 0.018518518518518517,\n",
       " 'draw': 0.018518518518518517,\n",
       " 'attention': 0.018518518518518517,\n",
       " 'risk': 0.05555555555555555,\n",
       " 'mitigation.[1][18': 0.018518518518518517,\n",
       " '\\u200a\\n\\n': 0.018518518518518517,\n",
       " 'performing': 0.018518518518518517,\n",
       " 'zero': 0.018518518518518517,\n",
       " 'shot': 0.05555555555555555,\n",
       " 'learning.[1': 0.018518518518518517,\n",
       " 'Reception[edit': 0.018518518518518517,\n",
       " 'Applications[edit': 0.018518518518518517,\n",
       " 'specifically': 0.018518518518518517,\n",
       " 'Codex': 0.018518518518518517,\n",
       " 'basis': 0.018518518518518517,\n",
       " 'GitHub': 0.018518518518518517,\n",
       " 'Copilot': 0.018518518518518517,\n",
       " 'code': 0.05555555555555555,\n",
       " 'completion': 0.018518518518518517,\n",
       " 'software': 0.018518518518518517,\n",
       " 'editors': 0.018518518518518517,\n",
       " 'IDEs': 0.018518518518518517,\n",
       " 'certain': 0.018518518518518517,\n",
       " 'products': 0.05555555555555555,\n",
       " 'translate': 0.018518518518518517,\n",
       " 'conventional': 0.018518518518518517,\n",
       " 'formal': 0.018518518518518517,\n",
       " 'computer': 0.05555555555555555,\n",
       " 'code.[19': 0.018518518518518517,\n",
       " 'Andrew': 0.018518518518518517,\n",
       " 'Mayne': 0.018518518518518517,\n",
       " 'Writer,[20': 0.018518518518518517,\n",
       " 'allows': 0.037037037037037035,\n",
       " 'people': 0.037037037037037035,\n",
       " 'correspond': 0.018518518518518517,\n",
       " 'historical': 0.018518518518518517,\n",
       " 'figures': 0.018518518518518517,\n",
       " 'email': 0.018518518518518517,\n",
       " 'Jason': 0.018518518518518517,\n",
       " 'Rohrer': 0.018518518518518517,\n",
       " 'retro': 0.018518518518518517,\n",
       " 'themed': 0.018518518518518517,\n",
       " 'chatbot': 0.037037037037037035,\n",
       " 'project': 0.018518518518518517,\n",
       " 'named': 0.018518518518518517,\n",
       " 'Project': 0.018518518518518517,\n",
       " 'December': 0.018518518518518517,\n",
       " 'accessible': 0.018518518518518517,\n",
       " 'online': 0.05555555555555555,\n",
       " 'converse': 0.018518518518518517,\n",
       " 'AIs': 0.018518518518518517,\n",
       " 'technology.[21': 0.018518518518518517,\n",
       " 'Guardian': 0.018518518518518517,\n",
       " 'write': 0.018518518518518517,\n",
       " 'article': 0.037037037037037035,\n",
       " 'harmless': 0.018518518518518517,\n",
       " 'beings': 0.018518518518518517,\n",
       " 'fed': 0.018518518518518517,\n",
       " 'ideas': 0.018518518518518517,\n",
       " 'different': 0.018518518518518517,\n",
       " 'essays': 0.018518518518518517,\n",
       " 'ultimately': 0.018518518518518517,\n",
       " 'merged': 0.018518518518518517,\n",
       " 'article.[22': 0.018518518518518517,\n",
       " 'Dungeon': 0.018518518518518517,\n",
       " 'adventure': 0.018518518518518517,\n",
       " 'games': 0.018518518518518517,\n",
       " 'Reviews[edit': 0.018518518518518517,\n",
       " 'review': 0.037037037037037035,\n",
       " 'New': 0.037037037037037035,\n",
       " 'York': 0.037037037037037035,\n",
       " 'Times': 0.037037037037037035,\n",
       " 'Farhad': 0.018518518518518517,\n",
       " 'Manjoo': 0.018518518518518517,\n",
       " 'said': 0.07407407407407407,\n",
       " 'ability': 0.018518518518518517,\n",
       " 'poetry': 0.018518518518518517,\n",
       " 'prose': 0.018518518518518517,\n",
       " 'amazing': 0.018518518518518517,\n",
       " 'spooky': 0.018518518518518517,\n",
       " 'humbling': 0.018518518518518517,\n",
       " 'little': 0.018518518518518517,\n",
       " 'terrifying\".[23': 0.018518518518518517,\n",
       " 'Daily': 0.018518518518518517,\n",
       " 'Nous': 0.018518518518518517,\n",
       " 'philosophers': 0.018518518518518517,\n",
       " 'GPT-3.[24': 0.018518518518518517,\n",
       " 'produced\".[5': 0.018518518518518517,\n",
       " 'Wired': 0.018518518518518517,\n",
       " 'provoking': 0.018518518518518517,\n",
       " 'chills': 0.018518518518518517,\n",
       " 'Silicon': 0.018518518518518517,\n",
       " 'Valley\".[25': 0.018518518518518517,\n",
       " 'National': 0.018518518518518517,\n",
       " 'Law': 0.018518518518518517,\n",
       " 'Review': 0.037037037037037035,\n",
       " 'impressive': 0.018518518518518517,\n",
       " 'step': 0.018518518518518517,\n",
       " 'finding': 0.018518518518518517,\n",
       " 'useful': 0.037037037037037035,\n",
       " 'power': 0.018518518518518517,\n",
       " 'continuing': 0.018518518518518517,\n",
       " 'work': 0.037037037037037035,\n",
       " 'intelligence\".[26': 0.018518518518518517,\n",
       " 'MIT': 0.018518518518518517,\n",
       " 'Technology': 0.018518518518518517,\n",
       " 'cowritten': 0.018518518518518517,\n",
       " 'Deep': 0.018518518518518517,\n",
       " 'Learning': 0.018518518518518517,\n",
       " 'critic': 0.018518518518518517,\n",
       " 'Gary': 0.018518518518518517,\n",
       " 'Marcus,[27': 0.018518518518518517,\n",
       " 'stated': 0.018518518518518517,\n",
       " 'comprehension': 0.018518518518518517,\n",
       " 'world': 0.018518518518518517,\n",
       " 'seriously': 0.018518518518518517,\n",
       " 'means': 0.018518518518518517,\n",
       " 'trust': 0.018518518518518517,\n",
       " 'says': 0.018518518518518517,\n",
       " '\"[28': 0.018518518518518517,\n",
       " 'relationships': 0.018518518518518517,\n",
       " 'having': 0.018518518518518517,\n",
       " 'meaning': 0.018518518518518517,\n",
       " 'Jerome': 0.018518518518518517,\n",
       " 'Pesenti': 0.018518518518518517,\n",
       " 'head': 0.018518518518518517,\n",
       " 'Facebook': 0.018518518518518517,\n",
       " 'lab': 0.018518518518518517,\n",
       " 'unsafe': 0.018518518518518517,\n",
       " 'pointing': 0.018518518518518517,\n",
       " 'sexist': 0.018518518518518517,\n",
       " 'racist': 0.018518518518518517,\n",
       " 'biased': 0.018518518518518517,\n",
       " 'negative': 0.018518518518518517,\n",
       " 'system': 0.018518518518518517,\n",
       " 'discuss': 0.018518518518518517,\n",
       " 'Jews': 0.018518518518518517,\n",
       " 'women': 0.018518518518518517,\n",
       " 'black': 0.018518518518518517,\n",
       " 'Holocaust.[29': 0.018518518518518517,\n",
       " 'Nabla': 0.018518518518518517,\n",
       " 'French': 0.018518518518518517,\n",
       " 'start': 0.018518518518518517,\n",
       " 'specializing': 0.018518518518518517,\n",
       " 'healthcare': 0.018518518518518517,\n",
       " 'technology': 0.018518518518518517,\n",
       " 'tested': 0.018518518518518517,\n",
       " 'medical': 0.018518518518518517,\n",
       " 'expected': 0.018518518518518517,\n",
       " 'showed': 0.018518518518518517,\n",
       " 'limitations': 0.018518518518518517,\n",
       " 'example': 0.037037037037037035,\n",
       " 'responses': 0.018518518518518517,\n",
       " 'mental': 0.018518518518518517,\n",
       " 'health': 0.018518518518518517,\n",
       " 'issues': 0.018518518518518517,\n",
       " 'advised': 0.018518518518518517,\n",
       " 'simulated': 0.018518518518518517,\n",
       " 'patient': 0.018518518518518517,\n",
       " 'commit': 0.018518518518518517,\n",
       " 'suicide.[30': 0.018518518518518517,\n",
       " 'Noam': 0.018518518518518517,\n",
       " 'Chomsky': 0.018518518518518517,\n",
       " 'expressed': 0.018518518518518517,\n",
       " 'skepticism': 0.018518518518518517,\n",
       " 'scientific': 0.05555555555555555,\n",
       " 'works': 0.018518518518518517,\n",
       " 'impossible': 0.018518518518518517,\n",
       " 'languages': 0.037037037037037035,\n",
       " 'actual': 0.018518518518518517,\n",
       " 'refuted': 0.018518518518518517,\n",
       " 'intended': 0.018518518518518517,\n",
       " 'normal': 0.018518518518518517,\n",
       " 'criteria': 0.018518518518518517,\n",
       " 'tell': 0.018518518518518517,\n",
       " 'cognition': 0.018518518518518517,\n",
       " 'generally': 0.018518518518518517,\n",
       " '\"[31': 0.018518518518518517,\n",
       " 'Luciano': 0.018518518518518517,\n",
       " 'Floridi': 0.018518518518518517,\n",
       " 'Massimo': 0.018518518518518517,\n",
       " 'Chiriatti': 0.018518518518518517,\n",
       " 'highlighted': 0.018518518518518517,\n",
       " 'cheap': 0.018518518518518517,\n",
       " 'production': 0.018518518518518517,\n",
       " 'semantic': 0.018518518518518517,\n",
       " 'artefacts\".[32': 0.018518518518518517,\n",
       " 'Criticism[edit': 0.018518518518518517,\n",
       " 'builder': 0.018518518518518517,\n",
       " 'initially': 0.018518518518518517,\n",
       " 'founded': 0.018518518518518517,\n",
       " 'profit': 0.037037037037037035,\n",
       " '2015.[33': 0.018518518518518517,\n",
       " '2019': 0.018518518518518517,\n",
       " 'publicly': 0.018518518518518517,\n",
       " 'precursor': 0.018518518518518517,\n",
       " 'breaking': 0.018518518518518517,\n",
       " 'previous': 0.018518518518518517,\n",
       " 'open': 0.018518518518518517,\n",
       " 'source': 0.037037037037037035,\n",
       " 'practices': 0.018518518518518517,\n",
       " 'citing': 0.018518518518518517,\n",
       " 'concerns': 0.037037037037037035,\n",
       " 'perpetuate': 0.018518518518518517,\n",
       " 'fake': 0.018518518518518517,\n",
       " 'eventually': 0.018518518518518517,\n",
       " 'released': 0.018518518518518517,\n",
       " 'size.[34': 0.018518518518518517,\n",
       " 'year': 0.018518518518518517,\n",
       " 'restructured': 0.018518518518518517,\n",
       " 'company.[35': 0.018518518518518517,\n",
       " 'company': 0.018518518518518517,\n",
       " 'licensing': 0.018518518518518517,\n",
       " 'services': 0.018518518518518517,\n",
       " 'following': 0.018518518518518517,\n",
       " 'multi': 0.018518518518518517,\n",
       " 'dollar': 0.018518518518518517,\n",
       " 'investment': 0.018518518518518517,\n",
       " 'agreement': 0.018518518518518517,\n",
       " 'permits': 0.018518518518518517,\n",
       " 'offer': 0.018518518518518517,\n",
       " 'facing': 0.018518518518518517,\n",
       " 'send': 0.018518518518518517,\n",
       " 'code.[6': 0.018518518518518517,\n",
       " 'Large': 0.018518518518518517,\n",
       " 'come': 0.018518518518518517,\n",
       " 'criticism': 0.018518518518518517,\n",
       " 'Google': 0.018518518518518517,\n",
       " 'ethics': 0.018518518518518517,\n",
       " 'environmental': 0.018518518518518517,\n",
       " 'impact': 0.018518518518518517,\n",
       " 'storing': 0.018518518518518517,\n",
       " 'detailed': 0.018518518518518517,\n",
       " 'co': 0.018518518518518517,\n",
       " 'authored': 0.018518518518518517,\n",
       " 'Timnit': 0.018518518518518517,\n",
       " 'Gebru': 0.018518518518518517,\n",
       " 'Emily': 0.018518518518518517,\n",
       " 'M.': 0.018518518518518517,\n",
       " 'Bender': 0.018518518518518517,\n",
       " '2021.[36': 0.018518518518518517,\n",
       " 'growing': 0.018518518518518517,\n",
       " 'automated': 0.018518518518518517,\n",
       " 'technologies': 0.018518518518518517,\n",
       " 'generators': 0.018518518518518517,\n",
       " 'raised': 0.037037037037037035,\n",
       " 'integrity[37': 0.018518518518518517,\n",
       " 'stakes': 0.018518518518518517,\n",
       " 'universities': 0.018518518518518517,\n",
       " 'schools': 0.018518518518518517,\n",
       " 'gauge': 0.018518518518518517,\n",
       " 'constitutes': 0.018518518518518517,\n",
       " 'misconduct': 0.018518518518518517,\n",
       " 'plagiarism.[38': 0.018518518518518517,\n",
       " 'criticized': 0.018518518518518517,\n",
       " 'algorithmic': 0.018518518518518517,\n",
       " 'bias': 0.018518518518518517,\n",
       " 'likely': 0.018518518518518517,\n",
       " 'associate': 0.018518518518518517,\n",
       " 'Islam': 0.018518518518518517,\n",
       " 'terrorism.[39': 0.018518518518518517,\n",
       " 'response': 0.018518518518518517,\n",
       " 'Request': 0.018518518518518517,\n",
       " 'Comments': 0.018518518518518517,\n",
       " 'Intellectual': 0.018518518518518517,\n",
       " 'Property': 0.018518518518518517,\n",
       " 'Protection': 0.018518518518518517,\n",
       " 'Artificial': 0.018518518518518517,\n",
       " 'Intelligence': 0.018518518518518517,\n",
       " 'Innovation': 0.018518518518518517,\n",
       " 'United': 0.018518518518518517,\n",
       " 'States': 0.018518518518518517,\n",
       " 'Patent': 0.018518518518518517,\n",
       " 'Trademark': 0.018518518518518517,\n",
       " 'Office': 0.018518518518518517,\n",
       " 'USPTO': 0.018518518518518517,\n",
       " 'acknowledges': 0.018518518518518517,\n",
       " 'copyright': 0.037037037037037035,\n",
       " 'protection': 0.018518518518518517,\n",
       " 'arises': 0.018518518518518517,\n",
       " 'automatically': 0.018518518518518517,\n",
       " 'author': 0.018518518518518517,\n",
       " 'creates': 0.018518518518518517,\n",
       " 'fixes': 0.018518518518518517,\n",
       " 'tangible': 0.018518518518518517,\n",
       " 'medium': 0.018518518518518517,\n",
       " 'U.S.C.': 0.018518518518518517,\n",
       " '102': 0.018518518518518517,\n",
       " 'vast': 0.018518518518518517,\n",
       " 'majority': 0.018518518518518517,\n",
       " 'content': 0.018518518518518517,\n",
       " 'protected': 0.018518518518518517,\n",
       " 'U.S.': 0.018518518518518517,\n",
       " 'laws.[40': 0.018518518518518517,\n",
       " 'built': 0.018518518518518517,\n",
       " 'conglomerate': 0.018518518518518517,\n",
       " 'copyrighted': 0.037037037037037035,\n",
       " 'internet': 0.018518518518518517,\n",
       " 'posts': 0.018518518518518517,\n",
       " 'web': 0.018518518518518517,\n",
       " 'pages': 0.018518518518518517,\n",
       " 'books': 0.037037037037037035,\n",
       " 'scraped': 0.018518518518518517,\n",
       " 'million': 0.018518518518518517,\n",
       " 'domains': 0.018518518518518517,\n",
       " 'period': 0.018518518518518517,\n",
       " 'years': 0.018518518518518517,\n",
       " 'TechCrunch': 0.018518518518518517,\n",
       " 'reports': 0.018518518518518517,\n",
       " 'includes': 0.018518518518518517,\n",
       " 'material': 0.018518518518518517,\n",
       " 'BBC': 0.018518518518518517,\n",
       " 'Reddit': 0.018518518518518517,\n",
       " 'more.[41': 0.018518518518518517,\n",
       " 'run': 0.018518518518518517,\n",
       " 'plagiarism': 0.018518518518518517,\n",
       " 'April': 0.018518518518518517,\n",
       " '2021': 0.018518518518518517,\n",
       " 'scientists': 0.018518518518518517,\n",
       " 'tool': 0.037037037037037035,\n",
       " 'identifies': 0.018518518518518517,\n",
       " 'effort': 0.018518518518518517,\n",
       " 'isolate': 0.018518518518518517,\n",
       " 'reason': 0.018518518518518517,\n",
       " 'strange': 0.018518518518518517,\n",
       " 'phrases': 0.018518518518518517,\n",
       " 'appearing': 0.018518518518518517,\n",
       " 'papers': 0.018518518518518517,\n",
       " 'Cabanac': 0.018518518518518517,\n",
       " 'colleagues': 0.018518518518518517,\n",
       " 'ran': 0.018518518518518517,\n",
       " 'selection': 0.018518518518518517,\n",
       " 'abstracts': 0.018518518518518517,\n",
       " 'Microprocessors': 0.018518518518518517,\n",
       " 'Microsystems': 0.018518518518518517,\n",
       " 'discovered': 0.018518518518518517,\n",
       " 'critical': 0.018518518518518517,\n",
       " 'flaws': 0.018518518518518517,\n",
       " 'nonsensical': 0.018518518518518517,\n",
       " 'plagiarized': 0.018518518518518517,\n",
       " 'images.[42': 0.018518518518518517}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9855ec",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cd1f26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text., \n",
      "\n",
      "It is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory.[2] GPT-3's full version has a capacity of 175 billion machine learning parameters., GPT-3, which was introduced in May 2020, and was in beta testing as of July 2020,[3] is part of a trend in natural language processing (NLP) systems of pre-trained language representations.[1]\n",
      "\n",
      "The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks.[4] Thirty-one OpenAI researchers and engineers presented the original May 28, 2020 paper introducing GPT-3., In their paper, they warned of GPT-3's potential dangers and called for research to mitigate risk.[1]: 34  David Chalmers, an Australian philosopher, described GPT-3 as \"one of the most interesting and important AI systems ever produced.\"[5]\n",
      "\n",
      "Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.[6]\n",
      "\n",
      "Further information: GPT-2 § Background\n",
      ", According to The Economist, improved algorithms, powerful computers, and an increase in digitized data have fueled a revolution in machine learning, with new techniques in the 2010s resulting in \"rapid improvements in tasks\" including manipulating language.[7] Software models are trained to learn by using thousands or millions of examples in a \"structure ... loosely based on the neural architecture of the brain\".[7] One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was first introduced in 2017—the Transformer.[8] GPT-n models are based on this Transformer-based deep learning neural network architecture., There are a number of NLP systems capable of processing, mining, organizing, connecting, contrasting, understanding and generating answers to questions.[9]\n",
      "\n",
      "On June 11, 2018, OpenAI researchers and engineers posted their original paper on generative models—language models—artificial intelligence systems—that could be pre-trained with an enormous and diverse corpus of text via datasets, in a process they called generative pre-training (GP).[10], The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of \"generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\", This eliminated the need for human supervision and for time-intensive hand-labeling.[10]\n",
      "\n",
      "In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was claimed to be the \"largest language model ever published at 17 billion parameters., \"[11] It performed better than any other language model at a variety of tasks which included summarizing texts and answering questions, despite being smaller than IBM's Tangora language model which had over 8 trillion parameters.[11]\n",
      "\n",
      "Training and capabilities[edit]\n",
      "On May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the development of GPT-3, a third-generation \"state-of-the-art language model\".[1][4], The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[12] making GPT-3 the largest non-sparse (in a sparse model, many of those parameters are set to the same constant value, so even if there are more total parameters, there is less meaningful information) language model to date.[1]: 14 [2] Because GPT-3 is structurally similar to its predecessors,[1] its higher level of accuracy is attributed to its increased capacity and higher number of parameters.[13] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model.[4]\n",
      "\n",
      "Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]: 9  GPT-3 was trained on hundreds of billions of words and is capable of coding in CSS, JSX, Python, among others.[3]\n",
      "\n",
      "GPT-3 Training Data\n",
      "Dataset\t# Tokens\tWeight in Training Mix\n",
      "Common Crawl\t410 billion\t60%\n",
      "WebText2\t19 billion\t22%\n",
      "Books1\t12 billion\t8%\n",
      "Books2\t55 billion\t8%\n",
      "Wikipedia\t3 billion\t3%\n",
      "Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[3] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data., A study from the University of Washington found that GPT-3 produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT-2 and CTRL., GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[14]\n",
      "\n",
      "On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology.[15][16] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[15] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[17], In an initial experiment 80 US subjects were asked to judge if short ~200 word articles were written by humans or GPT-3., The participants judged incorrectly 48% of the time, doing only slightly better than random guessing.[1]\n",
      "\n",
      "Because GPT-3 can \"generate news articles which human evaluators have difficulty distinguishing from articles written by humans,\"[4] GPT-3 has the \"potential to advance both the beneficial and harmful applications of language models., \"[1]: 34  In their May 28, 2020 paper, the researchers described in detail the potential \"harmful effects of GPT-3\"[4] which include \"misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting\".[1], The authors draw attention to these dangers to call for research on risk mitigation.[1][18]: 34 \n",
      "\n",
      "GPT-3 is capable of performing zero-shot, few-shot and one-shot learning.[1]\n",
      "\n",
      "Reception[edit]\n",
      "Applications[edit]\n",
      "GPT-3, specifically the Codex model, is the basis for GitHub Copilot, a code completion and generation software that can be used in various code editors and IDEs., \n",
      "GPT-3 is used in certain Microsoft products to translate conventional language into formal computer code.[19]\n",
      "GPT-3 has been used by Andrew Mayne for AI Writer,[20] which allows people to correspond with historical figures via email., \n",
      "GPT-3 has been used by Jason Rohrer in a retro-themed chatbot project named \"Project December\", which is accessible online and allows users to converse with several AIs using GPT-3 technology.[21]\n",
      "GPT-3 was used by The Guardian to write an article about AI being harmless to human beings., It was fed some ideas and produced eight different essays, which were ultimately merged into one article.[22]\n",
      "GPT-3 is used in AI Dungeon, which generates text-based adventure games., \n",
      "Reviews[edit]\n",
      "In a July 2020 review in The New York Times, Farhad Manjoo said that GPT-3's ability to generate computer code, poetry, and prose is not just \"amazing\", \"spooky\", and \"humbling\", but also \"more than a little terrifying\".[23]\n",
      "Daily Nous presented a series of articles by nine philosophers on GPT-3.[24] Australian philosopher David Chalmers described GPT-3 as \"one of the most interesting and important AI systems ever produced\".[5]\n",
      "A review in Wired said that GPT-3 was \"provoking chills across Silicon Valley\".[25]\n",
      "The National Law Review said that GPT-3 is an \"impressive step in the larger process\", with OpenAI and others finding \"useful applications for all of this power\" while continuing to \"work toward a more general intelligence\".[26]\n",
      "An article in the MIT Technology Review, cowritten by Deep Learning critic Gary Marcus,[27] stated that GPT-3's \"comprehension of the world is often seriously off, which means you can never really trust what it says., \"[28] According to the authors, GPT-3 models relationships between words without having an understanding of the meaning behind each word., \n",
      "Jerome Pesenti, head of the Facebook AI lab, said GPT-3 is \"unsafe,\" pointing to the sexist, racist and other biased and negative language generated by the system when it was asked to discuss Jews, women, black people, and the Holocaust.[29]\n",
      "Nabla, a French start-up specializing in healthcare technology, tested GPT-3 as a medical chatbot, though OpenAI itself warned against such use., As expected, GPT-3 showed several limitations., For example, while testing GPT-3 responses about mental health issues, the AI advised a simulated patient to commit suicide.[30]\n",
      "Noam Chomsky expressed his skepticism about GPT-3's scientific value: \"It's not a language model., It works just as well for impossible languages as for actual languages., It is therefore refuted, if intended as a language model, by normal scientific criteria., [...] Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition generally., \"[31]\n",
      "Luciano Floridi and Massimo Chiriatti highlighted the risk of \"cheap production of good, semantic artefacts\".[32]\n",
      "Criticism[edit]\n",
      "GPT-3's builder, OpenAI, was initially founded as a non-profit in 2015.[33], In 2019, OpenAI did not publicly release GPT-3's precursor model, breaking from OpenAI's previous open-source practices, citing concerns that the model would perpetuate fake news., OpenAI eventually released a version of GPT-2 that was 8% of the original model's size.[34], In the same year, OpenAI restructured to be a for-profit company.[35], In 2020, Microsoft announced the company had exclusive licensing of GPT-3 for Microsoft's products and services following a multi-billion dollar investment in OpenAI., The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to the GPT-3's source code.[6]\n",
      "\n",
      "Large language models, such as GPT-3, have come under criticism from Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[36]\n",
      "\n",
      "The growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[37] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[38]\n",
      "\n",
      "GPT-3 was criticized for its algorithmic bias; for example, it is more likely to associate Islam with terrorism.[39]\n",
      "\n",
      ", In its response to the Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office (\"USPTO\"), OpenAI acknowledges that \"copyright protection arises automatically when an author creates an original work and fixes it in a tangible medium, see 17 U.S.C. § 102, the vast majority of content posted online is protected by U.S. copyright laws.[40] However, GPT was built with data from the Common Crawl dataset, a conglomerate of copyrighted articles, internet posts, web pages, and books scraped from 60 million domains over a period of 12 years., TechCrunch reports this data includes copyrighted material from BBC, The New York Times, Reddit, the full text of online books, and more.[41] All GPT-based products therefore run the risk of plagiarism., In April 2021, a group of computer scientists used a tool that identifies text generated by GPT in an effort to isolate the reason for strange phrases appearing in scientific papers., Cabanac and colleagues ran a selection of abstracts from Microprocessors and Microsystems through this tool and discovered \"critical flaws\", such as nonsensical text and plagiarized text and images.[42]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sent_tokens=[sent for sent in doc.sents]\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445172ed",
   "metadata": {},
   "source": [
    "### Calculating sentence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc87fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_score={}\n",
    "for sent in sent_tokens:\n",
    "    for word in sent:\n",
    "        if word.text.lower() in word_freq.keys():\n",
    "            if sent not in sentence_score.keys():\n",
    "                sentence_score[sent]=word_freq[word.text.lower()]\n",
    "            else:\n",
    "                sentence_score[sent]+=word_freq[word.text.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e048ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model that uses deep learning to produce human-like text.: 1.814814814814815, \n",
      "\n",
      "It is the third-generation language prediction model in the GPT-n series (and the successor to GPT-2) created by OpenAI, a San Francisco-based artificial intelligence research laboratory.[2] GPT-3's full version has a capacity of 175 billion machine learning parameters.: 2.3333333333333335, GPT-3, which was introduced in May 2020, and was in beta testing as of July 2020,[3] is part of a trend in natural language processing (NLP) systems of pre-trained language representations.[1]\n",
      "\n",
      "The quality of the text generated by GPT-3 is so high that it can be difficult to determine whether or not it was written by a human, which has both benefits and risks.[4] Thirty-one OpenAI researchers and engineers presented the original May 28, 2020 paper introducing GPT-3.: 3.388888888888889, In their paper, they warned of GPT-3's potential dangers and called for research to mitigate risk.[1]: 34  David Chalmers, an Australian philosopher, described GPT-3 as \"one of the most interesting and important AI systems ever produced.\"[5]\n",
      "\n",
      "Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.[6]\n",
      "\n",
      "Further information: GPT-2 § Background\n",
      ": 2.4444444444444446, According to The Economist, improved algorithms, powerful computers, and an increase in digitized data have fueled a revolution in machine learning, with new techniques in the 2010s resulting in \"rapid improvements in tasks\" including manipulating language.[7] Software models are trained to learn by using thousands or millions of examples in a \"structure ... loosely based on the neural architecture of the brain\".[7] One architecture used in natural language processing (NLP) is a neural network based on a deep learning model that was first introduced in 2017—the Transformer.[8] GPT-n models are based on this Transformer-based deep learning neural network architecture.: 3.9259259259259243, There are a number of NLP systems capable of processing, mining, organizing, connecting, contrasting, understanding and generating answers to questions.[9]\n",
      "\n",
      "On June 11, 2018, OpenAI researchers and engineers posted their original paper on generative models—language models—artificial intelligence systems—that could be pre-trained with an enormous and diverse corpus of text via datasets, in a process they called generative pre-training (GP).[10]: 3.518518518518519, The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of \"generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\": 3.3703703703703716, This eliminated the need for human supervision and for time-intensive hand-labeling.[10]\n",
      "\n",
      "In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was claimed to be the \"largest language model ever published at 17 billion parameters.: 2.8148148148148153, \"[11] It performed better than any other language model at a variety of tasks which included summarizing texts and answering questions, despite being smaller than IBM's Tangora language model which had over 8 trillion parameters.[11]\n",
      "\n",
      "Training and capabilities[edit]\n",
      "On May 28, 2020, an arXiv preprint by a group of 31 engineers and researchers at OpenAI described the development of GPT-3, a third-generation \"state-of-the-art language model\".[1][4]: 3.9444444444444455, The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[12] making GPT-3 the largest non-sparse (in a sparse model, many of those parameters are set to the same constant value, so even if there are more total parameters, there is less meaningful information) language model to date.[1]: 14 [2] Because GPT-3 is structurally similar to its predecessors,[1] its higher level of accuracy is attributed to its increased capacity and higher number of parameters.[13] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model.[4]\n",
      "\n",
      "Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]: 9  GPT-3 was trained on hundreds of billions of words and is capable of coding in CSS, JSX, Python, among others.[3]\n",
      "\n",
      "GPT-3 Training Data\n",
      "Dataset\t# Tokens\tWeight in Training Mix\n",
      "Common Crawl\t410 billion\t60%\n",
      "WebText2\t19 billion\t22%\n",
      "Books1\t12 billion\t8%\n",
      "Books2\t55 billion\t8%\n",
      "Wikipedia\t3 billion\t3%\n",
      "Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[3] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data.: 15.240740740740739, A study from the University of Washington found that GPT-3 produced toxic language at a toxicity level comparable to the similar natural language processing models of GPT-2 and CTRL.: 1.8333333333333333, GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[14]\n",
      "\n",
      "On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology.[15][16] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[15] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[17]: 6.4074074074074, In an initial experiment 80 US subjects were asked to judge if short ~200 word articles were written by humans or GPT-3.: 0.37037037037037035, The participants judged incorrectly 48% of the time, doing only slightly better than random guessing.[1]\n",
      "\n",
      "Because GPT-3 can \"generate news articles which human evaluators have difficulty distinguishing from articles written by humans,\"[4] GPT-3 has the \"potential to advance both the beneficial and harmful applications of language models.: 1.8888888888888886, \"[1]: 34  In their May 28, 2020 paper, the researchers described in detail the potential \"harmful effects of GPT-3\"[4] which include \"misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting\".[1]: 1.2777777777777783, The authors draw attention to these dangers to call for research on risk mitigation.[1][18]: 34 \n",
      "\n",
      "GPT-3 is capable of performing zero-shot, few-shot and one-shot learning.[1]\n",
      "\n",
      "Reception[edit]\n",
      "Applications[edit]\n",
      "GPT-3, specifically the Codex model, is the basis for GitHub Copilot, a code completion and generation software that can be used in various code editors and IDEs.: 1.62962962962963, \n",
      "GPT-3 is used in certain Microsoft products to translate conventional language into formal computer code.[19]\n",
      "GPT-3 has been used by Andrew Mayne for AI Writer,[20] which allows people to correspond with historical figures via email.: 0.9259259259259256, \n",
      "GPT-3 has been used by Jason Rohrer in a retro-themed chatbot project named \"Project December\", which is accessible online and allows users to converse with several AIs using GPT-3 technology.[21]\n",
      "GPT-3 was used by The Guardian to write an article about AI being harmless to human beings.: 0.5185185185185184, It was fed some ideas and produced eight different essays, which were ultimately merged into one article.[22]\n",
      "GPT-3 is used in AI Dungeon, which generates text-based adventure games.: 0.6851851851851851, \n",
      "Reviews[edit]\n",
      "In a July 2020 review in The New York Times, Farhad Manjoo said that GPT-3's ability to generate computer code, poetry, and prose is not just \"amazing\", \"spooky\", and \"humbling\", but also \"more than a little terrifying\".[23]\n",
      "Daily Nous presented a series of articles by nine philosophers on GPT-3.[24] Australian philosopher David Chalmers described GPT-3 as \"one of the most interesting and important AI systems ever produced\".[5]\n",
      "A review in Wired said that GPT-3 was \"provoking chills across Silicon Valley\".[25]\n",
      "The National Law Review said that GPT-3 is an \"impressive step in the larger process\", with OpenAI and others finding \"useful applications for all of this power\" while continuing to \"work toward a more general intelligence\".[26]\n",
      "An article in the MIT Technology Review, cowritten by Deep Learning critic Gary Marcus,[27] stated that GPT-3's \"comprehension of the world is often seriously off, which means you can never really trust what it says.: 2.185185185185186, \"[28] According to the authors, GPT-3 models relationships between words without having an understanding of the meaning behind each word.: 0.4259259259259259, \n",
      "Jerome Pesenti, head of the Facebook AI lab, said GPT-3 is \"unsafe,\" pointing to the sexist, racist and other biased and negative language generated by the system when it was asked to discuss Jews, women, black people, and the Holocaust.[29]\n",
      "Nabla, a French start-up specializing in healthcare technology, tested GPT-3 as a medical chatbot, though OpenAI itself warned against such use.: 1.2777777777777781, As expected, GPT-3 showed several limitations.: 0.05555555555555555, For example, while testing GPT-3 responses about mental health issues, the AI advised a simulated patient to commit suicide.[30]\n",
      "Noam Chomsky expressed his skepticism about GPT-3's scientific value: \"It's not a language model.: 1.2777777777777777, It works just as well for impossible languages as for actual languages.: 0.12962962962962962, It is therefore refuted, if intended as a language model, by normal scientific criteria.: 1.037037037037037, [...] Perhaps it's useful for some purpose, but it seems to tell us nothing about language or cognition generally.: 0.7407407407407407, \"[31]\n",
      "Luciano Floridi and Massimo Chiriatti highlighted the risk of \"cheap production of good, semantic artefacts\".[32]\n",
      "Criticism[edit]\n",
      "GPT-3's builder, OpenAI, was initially founded as a non-profit in 2015.[33]: 0.35185185185185186, In 2019, OpenAI did not publicly release GPT-3's precursor model, breaking from OpenAI's previous open-source practices, citing concerns that the model would perpetuate fake news.: 0.9999999999999996, OpenAI eventually released a version of GPT-2 that was 8% of the original model's size.[34]: 0.6296296296296297, In the same year, OpenAI restructured to be a for-profit company.[35]: 0.09259259259259259, In 2020, Microsoft announced the company had exclusive licensing of GPT-3 for Microsoft's products and services following a multi-billion dollar investment in OpenAI.: 0.685185185185185, The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to the GPT-3's source code.[6]\n",
      "\n",
      "Large language models, such as GPT-3, have come under criticism from Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[36]\n",
      "\n",
      "The growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[37] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[38]\n",
      "\n",
      "GPT-3 was criticized for its algorithmic bias; for example, it is more likely to associate Islam with terrorism.[39]\n",
      "\n",
      ": 5.037037037037033, In its response to the Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation from the United States Patent and Trademark Office (\"USPTO\"), OpenAI acknowledges that \"copyright protection arises automatically when an author creates an original work and fixes it in a tangible medium, see 17 U.S.C. § 102, the vast majority of content posted online is protected by U.S. copyright laws.[40] However, GPT was built with data from the Common Crawl dataset, a conglomerate of copyrighted articles, internet posts, web pages, and books scraped from 60 million domains over a period of 12 years.: 1.370370370370371, TechCrunch reports this data includes copyrighted material from BBC, The New York Times, Reddit, the full text of online books, and more.[41] All GPT-based products therefore run the risk of plagiarism.: 0.9074074074074073, In April 2021, a group of computer scientists used a tool that identifies text generated by GPT in an effort to isolate the reason for strange phrases appearing in scientific papers.: 0.6666666666666665, Cabanac and colleagues ran a selection of abstracts from Microprocessors and Microsystems through this tool and discovered \"critical flaws\", such as nonsensical text and plagiarized text and images.[42]\n",
      ": 0.7037037037037036}\n"
     ]
    }
   ],
   "source": [
    "print(sentence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ecff94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d02f59ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_len=int(len(sent_tokens)*0.1)\n",
    "select_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46b88902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[12] making GPT-3 the largest non-sparse (in a sparse model, many of those parameters are set to the same constant value, so even if there are more total parameters, there is less meaningful information) language model to date.[1]: 14 [2] Because GPT-3 is structurally similar to its predecessors,[1] its higher level of accuracy is attributed to its increased capacity and higher number of parameters.[13] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model.[4]\n",
       " \n",
       " Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]: 9  GPT-3 was trained on hundreds of billions of words and is capable of coding in CSS, JSX, Python, among others.[3]\n",
       " \n",
       " GPT-3 Training Data\n",
       " Dataset\t# Tokens\tWeight in Training Mix\n",
       " Common Crawl\t410 billion\t60%\n",
       " WebText2\t19 billion\t22%\n",
       " Books1\t12 billion\t8%\n",
       " Books2\t55 billion\t8%\n",
       " Wikipedia\t3 billion\t3%\n",
       " Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[3] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data.,\n",
       " GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[14]\n",
       " \n",
       " On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology.[15][16] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[15] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[17],\n",
       " The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to the GPT-3's source code.[6]\n",
       " \n",
       " Large language models, such as GPT-3, have come under criticism from Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[36]\n",
       " \n",
       " The growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[37] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[38]\n",
       " \n",
       " GPT-3 was criticized for its algorithmic bias; for example, it is more likely to associate Islam with terrorism.[39]\n",
       " ]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary=nlargest(select_len,sentence_score,key=sentence_score.get)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0668928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[12] making GPT-3 the largest non-sparse (in a sparse model, many of those parameters are set to the same constant value, so even if there are more total parameters, there is less meaningful information) language model to date.[1]:\\u200a14\\u200a[2] Because GPT-3 is structurally similar to its predecessors,[1] its higher level of accuracy is attributed to its increased capacity and higher number of parameters.[13] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model.[4]\\n\\nSixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]:\\u200a9\\u200a Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]:\\u200a9\\u200a GPT-3 was trained on hundreds of billions of words and is capable of coding in CSS, JSX, Python, among others.[3]\\n\\nGPT-3 Training Data\\nDataset\\t# Tokens\\tWeight in Training Mix\\nCommon Crawl\\t410 billion\\t60%\\nWebText2\\t19 billion\\t22%\\nBooks1\\t12 billion\\t8%\\nBooks2\\t55 billion\\t8%\\nWikipedia\\t3 billion\\t3%\\nSince GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[3] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data.\",\n",
       " 'GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[14]\\n\\nOn June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology.[15][16] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[15] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[17]',\n",
       " \"The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to the GPT-3's source code.[6]\\n\\nLarge language models, such as GPT-3, have come under criticism from Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[36]\\n\\nThe growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[37] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[38]\\n\\nGPT-3 was criticized for its algorithmic bias; for example, it is more likely to associate Islam with terrorism.[39]\\n\\n\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary=[word.text for word in summary]\n",
    "final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561d3f6",
   "metadata": {},
   "source": [
    "### Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e97509e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The team increased the capacity of GPT-3 by over two orders of magnitude from that of its predecessor, GPT-2,[12] making GPT-3 the largest non-sparse (in a sparse model, many of those parameters are set to the same constant value, so even if there are more total parameters, there is less meaningful information) language model to date.[1]: 14 [2] Because GPT-3 is structurally similar to its predecessors,[1] its higher level of accuracy is attributed to its increased capacity and higher number of parameters.[13] GPT-3's capacity is ten times larger than that of Microsoft's Turing NLG, the next largest NLP model.[4]\n",
      "\n",
      "Sixty percent of the weighted pre-training dataset for GPT-3 comes from a filtered version of Common Crawl consisting of 410 billion byte-pair-encoded tokens.[1]: 9  Other sources are 19 billion tokens from WebText2 representing 22% of the weighted total, 12 billion tokens from Books1 representing 8%, 55 billion tokens from Books2 representing 8%, and 3 billion tokens from Wikipedia representing 3%.[1]: 9  GPT-3 was trained on hundreds of billions of words and is capable of coding in CSS, JSX, Python, among others.[3]\n",
      "\n",
      "GPT-3 Training Data\n",
      "Dataset\t# Tokens\tWeight in Training Mix\n",
      "Common Crawl\t410 billion\t60%\n",
      "WebText2\t19 billion\t22%\n",
      "Books1\t12 billion\t8%\n",
      "Books2\t55 billion\t8%\n",
      "Wikipedia\t3 billion\t3%\n",
      "Since GPT-3's training data was all-encompassing, it does not require further training for distinct language tasks.[3] The training data contains occasional toxic language and GPT-3 occasionally generates toxic language as a result of mimicking its training data.GPT-3 produced less toxic language compared to its predecessor model, GPT-1, although it produced both more generations and a higher toxicity of toxic language compared to CTRL Wiki, a language model trained entirely on Wikipedia data.[14]\n",
      "\n",
      "On June 11, 2020, OpenAI announced that users could request access to its user-friendly GPT-3 API—a \"machine learning toolset\"—to help OpenAI \"explore the strengths and limits\" of this new technology.[15][16] The invitation described how this API had a general-purpose \"text in, text out\" interface that can complete almost \"any English language task\", instead of the usual single use-case.[15] According to one user, who had access to a private early release of the OpenAI GPT-3 API, GPT-3 was \"eerily good\" at writing \"amazingly coherent text\" with only a few simple prompts.[17]The agreement permits OpenAI to offer a public-facing API such that users can send text to GPT-3 to receive the model's output, but only Microsoft will have access to the GPT-3's source code.[6]\n",
      "\n",
      "Large language models, such as GPT-3, have come under criticism from Google's AI ethics researchers for the environmental impact of training and storing the models, detailed in a paper co-authored by Timnit Gebru and Emily M. Bender in 2021.[36]\n",
      "\n",
      "The growing use of automated writing technologies based on GPT-3 and other language generators, has raised concerns regarding academic integrity[37] and raised the stakes of how universities and schools will gauge what constitutes academic misconduct such as plagiarism.[38]\n",
      "\n",
      "GPT-3 was criticized for its algorithmic bias; for example, it is more likely to associate Islam with terrorism.[39]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summ=''.join(final_summary)\n",
    "print(summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8548bf9",
   "metadata": {},
   "source": [
    "### Length of summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4395b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3247"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd49e4",
   "metadata": {},
   "source": [
    "### Length of original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ba30c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12744"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
